{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde454bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init import *\n",
    "from testURL import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107dfca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/mdjavedlferdous/Documents/Dataset/Testing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875b22e",
   "metadata": {},
   "source": [
    "### Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778128aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from init import *\n",
    "from allURL import *\n",
    "\n",
    "def search(myDict, search1):\n",
    "    search.a=[]\n",
    "    for key, value in myDict.items():\n",
    "        if search1 in value:\n",
    "            search.a.append(key)\n",
    "    return len(search.a)\n",
    "\n",
    "def search_query(_url_):\n",
    "    s_inner,is_button,search_attribute, noWord, sClass,search_button_attribute_value  = ([] for i in range(6)) \n",
    "    BnoWord = 0\n",
    "    myFile=open(_url_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    try:\n",
    "        for tests in soup.findAll('form'):\n",
    "            if('search' in list(tests.attrs.values())):\n",
    "                search_attribute.append(1)\n",
    "            else:\n",
    "                search_attribute.append(0) \n",
    "            if \" \" == tests.text:\n",
    "                s_inner.append(0)\n",
    "                \n",
    "            else:\n",
    "                s_inner.append(1)                \n",
    "            my_no = tests.attrs\n",
    "            noWord.append(search(my_no, \"search\"))\n",
    "            \n",
    "            is_present = bool(re.search('button', str(tests)))\n",
    "            if(is_present == True):\n",
    "                is_button.append(1)\n",
    "            else:\n",
    "                is_button.append(0)\n",
    "                \n",
    "            buttonSearch = tests.find(\"button\")\n",
    "            button_attributes = buttonSearch.attrs\n",
    "            BnoWord = search(button_attributes, \"search\")\n",
    "            if(BnoWord == 1):\n",
    "                search_button_attribute_value.append(1)\n",
    "            else:\n",
    "                search_button_attribute_value.append(0)\n",
    "  \n",
    "            \n",
    "    except:\n",
    "        search_button_attribute_value.append(0)\n",
    "    \n",
    "    if (s_inner ==[]):\n",
    "        s_inner = [0]\n",
    "    if (search_attribute ==[]):\n",
    "        search_attribute = [0]\n",
    "    if (noWord ==[]):\n",
    "        noWord = [0]\n",
    "    if (is_button ==[]):\n",
    "        is_button = [0]\n",
    "    if (search_button_attribute_value ==[]):\n",
    "        search_button_attribute_value = [0]    \n",
    "    if (sClass ==[]):\n",
    "        sClass = [1]\n",
    "    \n",
    "    temp = len(s_inner)*[_url_]\n",
    "    \n",
    "    return temp,s_inner, search_attribute,noWord,is_button, search_button_attribute_value\n",
    "# for i in range(100):\n",
    "#     print([i],search_query(turl[i]))\n",
    "#print(search_query(turl[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_data(searchQ) :\n",
    "        name_url, search_innertext,search_attribute,noWord, is_button, search_button_attribute_value = search_query(searchQ)\n",
    "        print(name_url, search_innertext,search_attribute,noWord, is_button, search_button_attribute_value)\n",
    "        search_innertext = str(search_innertext)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        search_attribute = str(search_attribute)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        noWord = str(noWord)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        is_button = str(is_button)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        search_button_attribute_value = str(search_button_attribute_value)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        temp = []\n",
    "        temp2 = []\n",
    "        \n",
    "        for i in search_innertext:\n",
    "            search_innertext = i\n",
    "            temp.append([search_innertext])        \n",
    "            \n",
    "        t_search_attribute = []\n",
    "        for m in search_attribute:\n",
    "            t_search_attribute.append(m)\n",
    "        arr2d = np.matrix(temp)\n",
    "        column_to_add = np.array(t_search_attribute)\n",
    "        output = np.column_stack((arr2d, column_to_add))\n",
    "        f_search_attribute = output.tolist()\n",
    "        \n",
    "        t_noWord = []\n",
    "        for m in noWord:\n",
    "            t_noWord.append(m)\n",
    "        a_noWord = np.matrix(f_search_attribute)\n",
    "        column_noWord = np.array(t_noWord)\n",
    "        o_noWord = np.column_stack((a_noWord, column_noWord))\n",
    "        f_noWord = o_noWord.tolist()        \n",
    "        \n",
    "        t_is_button= []\n",
    "        for m in is_button:\n",
    "            t_is_button.append(m)\n",
    "        a_is_button = np.matrix(f_noWord)\n",
    "        column_is_button= np.array(t_is_button)\n",
    "        o_is_button = np.column_stack((a_is_button, column_is_button))\n",
    "        f_is_button= o_is_button.tolist()        \n",
    "        \n",
    "        t_s_value= []\n",
    "        for m in search_button_attribute_value:\n",
    "            t_s_value.append(m)\n",
    "        a_s_value = np.matrix(f_is_button)\n",
    "        column_s_value= np.array(t_s_value)\n",
    "        o_s_value = np.column_stack((a_s_value, column_s_value))\n",
    "        f_s_value= o_s_value.tolist()\n",
    "        \n",
    "        t_name= []\n",
    "        for m in name_url:\n",
    "            t_name.append(m)\n",
    "        a_name = np.matrix(f_s_value)\n",
    "        column_name = np.array(t_name)\n",
    "        o_name = np.column_stack((a_name, column_name))\n",
    "        f_name= o_name.tolist()\n",
    "        \n",
    "        return f_name\n",
    "get_class_data(turl[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb875fa9",
   "metadata": {},
   "source": [
    "### Export  to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5828c8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_header():\n",
    "    list_of_header = [\"search_innertext\", \"search_attribute\", \"Number_of_search_word\",\"search_button_attribute_value\",\"is_button\",  \"URL name\"]\n",
    "    save_path = 'result/search/'\n",
    "    file_name = \"SearchList_test__1.csv\"\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "\n",
    "    with open(completeName, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list_of_header)\n",
    "\n",
    "def write_CSV(tlist):\n",
    "    save_path = 'result/search/'\n",
    "    file_name = \"SearchList_test__1.csv\"\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "\n",
    "    with open(completeName, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(tlist)\n",
    "    with open(completeName, \"r\", newline=\"\") as fr:\n",
    "        reader = csv.reader(fr)\n",
    "        lines= len(list(reader))\n",
    "        print(\"[\",lines,\"].\", \"form!\")\n",
    "\n",
    "def main():\n",
    "    write_header()\n",
    "    for i in range(0,100):\n",
    "            print(write_CSV(get_class_data(turl[i])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883baa61",
   "metadata": {},
   "source": [
    "#### Split all the webpage into separate CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./result/search/SearchList_test__1.csv\")\n",
    "\n",
    "for (gender), group in data.groupby(['URL name']):\n",
    "     group.to_csv(f'{gender}.csv', index=False)\n",
    "\n",
    "# for i in range(0,1):\n",
    "#     namecsv = turl[i]+\".csv\"\n",
    "# print(pd.read_csv(namecsv))\n",
    "# # print(pd.read_csv(\"/Users/mdjavedulferdous/Desktop/TiiS/Code/result/search/Ajio.html.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192efb0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Import libraries\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "\n",
    "# # Get CSV files list from a folder\n",
    "# path = '/Users/mdjavedulferdous/Desktop/TiiS/Code/result/search/model_csv'\n",
    "# csv_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# # Read each CSV file into DataFrame\n",
    "# # This creates a list of dataframes\n",
    "# for file in csv_files:\n",
    "#     Search_csv_df = pd.read_csv(file)\n",
    "#     search_URL_name = Search_csv_df[[\"URL name\"]]\n",
    "#     nameCSV = (search_URL_name[\"URL name\"][0].split('/')[-1]).split('.')[0]\n",
    "#     nameCSV = nameCSV+'.csv'\n",
    "#     Search_X_test = Search_csv_df[[\"search_innertext\", \"search_attribute\", \"Number_of_search_word\",\"search_button_attribute_value\",\"is_button\"]]\n",
    "#     searchname = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/search_model.sav\"\n",
    "#     target_names = ['Non-Search', 'Search']\n",
    "#     data1_import = evaluation(Search_X_test,searchname,target_names,search_URL_name)\n",
    "#     data2_import = pd.read_csv(Search_searchpath)   \n",
    "#     data_merge = pd.concat([data1_import, data2_import],axis=1)\n",
    "#     data_merge.to_csv(nameCSV, index = False)  # Export merged pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988607e0",
   "metadata": {},
   "source": [
    "#### Compare with GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b32231",
   "metadata": {},
   "outputs": [],
   "source": [
    "GTdata = pd.read_csv(\"./result/search/search_GT__1.csv\")\n",
    "extractedFrame = pd.read_csv(\"/Users/mdjavedulferdous/Desktop/TiiS/Code/search_after _model/1mg.csv\")\n",
    "for i in range(len(GTdata['URL name'])):\n",
    "    if  any(GTdata['URL name'][i] == extractedFrame['URL name']):\n",
    "        if (extractedFrame['0'].item() < extractedFrame['1'].item()) and GTdata['sClass'][i] ==1 :\n",
    "            print(\"TRUE Prediction\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d458e",
   "metadata": {},
   "source": [
    "### Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b030e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pageListFunction(_url_):\n",
    "    myFile=open(_url_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    \n",
    "    divPageList, navPageList,liPageList,ulPageList,spanPageList, sectionPageList, buttonPageList, \\\n",
    "    trPageList, footerPageList, aPageList, paginationPageList, bPageList \\\n",
    "    = soup.findAll('div'), soup.findAll('nav'), \\\n",
    "    soup.findAll('li'), soup.findAll('ul'),\\\n",
    "    soup.findAll('span'), soup.findAll('section'), \\\n",
    "    soup.findAll('button'), soup.findAll('tr'), \\\n",
    "    soup.findAll('footer'), soup.findAll('a'), \\\n",
    "    soup.findAll('pagination'), soup.findAll('b')\n",
    "    \n",
    "    try:\n",
    "        if divPageList != []:    \n",
    "            return pageList_Extract(soup, 'div',_url_)\n",
    "        \n",
    "        if navPageList != []:    \n",
    "            return pageList_Extract(soup, 'nav',_url_)\n",
    "        \n",
    "        if liPageList != []: \n",
    "            return pageList_Extract(soup, 'li',_url_)\n",
    "        \n",
    "        if ulPageList != []: \n",
    "            return pageList_Extract(soup, 'ul',_url_)\n",
    "        \n",
    "        if spanPageList != []:  \n",
    "            return pageList_Extract(soup, 'span',_url_)\n",
    "        \n",
    "        if sectionPageList != []: \n",
    "            return pageList_Extract(soup, 'section',_url_) \n",
    "        \n",
    "        if buttonPageList != []: \n",
    "            return pageList_Extract(soup, 'button',_url_)\n",
    "        \n",
    "        if trPageList != []: \n",
    "            return pageList_Extract(soup, 'tr',_url_) \n",
    "        \n",
    "        if footerPageList != []: \n",
    "            return pageList_Extract(soup, 'footer',_url_) \n",
    "        \n",
    "        if aPageList != []: \n",
    "            return pageList_Extract(soup, 'a',_url_) \n",
    "        \n",
    "        if paginationPageList != []: \n",
    "            \n",
    "            return pageList_Extract(soup, 'pagination',_url_) \n",
    "        \n",
    "        if bPageList != []: \n",
    "            return  pageList_Extract(soup, 'b',_url_)  \n",
    "        else:\n",
    "            return _url_,\"0\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\"\n",
    "    except:\n",
    "        pass\n",
    "def search(myDict, search1):\n",
    "    search.a=[]\n",
    "    for key, value in myDict.items():\n",
    "        if search1 in value:\n",
    "            search.a.append(key)\n",
    "    return len(search.a)\n",
    "\n",
    "\n",
    "def pageList_Extract(soup, tag,_url_):  \n",
    "    pageClass, NumOfPage, pageListAttribute, is_page, NumOfButton, NumOfLinks, NumberOfValues, outsideURL, insideURL, commonURL, \\\n",
    "    navType= [] ,[], [],[], [], [], [], [], [], [], []\n",
    "    \n",
    "    print(\"Tag: \", tag)\n",
    "    for ele in soup.findAll(tag):\n",
    "        try:\n",
    "                count, btn, valueCounter = 0, 0, 0\n",
    "                nText = ele.text\n",
    "                #=======================Page Name======================= \n",
    "                is_present = bool(re.search('page', str(ele)) or re.search('show', str(ele)))\n",
    "                if(is_present == True):\n",
    "                    is_page.append(1)\n",
    "                else:\n",
    "                    is_page.append(0)\n",
    "                #====================Number of links==================== \n",
    "                for link in ele.find_all('a'):\n",
    "                    count += 1\n",
    "                NumOfLinks.append(count)\n",
    "                #====================Number of Button==================== \n",
    "                for btnlink in ele.find_all('button'):\n",
    "                    btn += 1\n",
    "                NumOfButton.append(btn)\n",
    "                #=======================Common url=======================\n",
    "                insideURL, outsideURL = [], []\n",
    "                for link in ele.find_all('a'):\n",
    "                    insideURL.append(link.get('href'))\n",
    "                for link in soup.find_all('a'):\n",
    "                    outsideURL.append(link.get('href'))\n",
    "                def compare(list1,list2):\n",
    "                    ln= []\n",
    "                    for i in list1:\n",
    "                        if i  in list2:\n",
    "                           ln.append(i)\n",
    "                    return ln\n",
    "                s = -len(set(compare(insideURL,outsideURL)))+len(outsideURL)\n",
    "                commonURL.append(s)\n",
    "                \n",
    "                #=====================Number of values=====================\n",
    "                for link in ele.find_all('a'):\n",
    "                    if (link.text).isdigit()==True:\n",
    "                        valueCounter += 1\n",
    "                NumberOfValues.append(valueCounter)\n",
    "                #print(valueCounter)\n",
    "                \n",
    "                #=====================Number of pages=====================\n",
    "                temp1 = re.findall(r'\\d+', ele.text) \n",
    "                res2 = list(map(int, temp1))\n",
    "                if(len(res2)==1):\n",
    "                    if res2==[0] or res2==[]:\n",
    "                        NumOfPage.append(1)\n",
    "                    else:\n",
    "                        NumOfPage.append(str(res2[0])[0:2])\n",
    "                else:\n",
    "                    if res2==[0] or res2==[]:\n",
    "                        NumOfPage.append(1)\n",
    "                    else:\n",
    "                        NumOfPage.append(str(res2[-1])[0:2])\n",
    "                #=====================Navigation type=====================\n",
    "                nav = ele.find('button')\n",
    "                alink = ele.find('a')\n",
    "                if (bool(nav)) == True:\n",
    "                    if(bool(alink)) == True and (bool(nav)) == True:\n",
    "                        navType.append(3)\n",
    "                    else:\n",
    "                        navType.append(1)\n",
    "                elif(bool(alink)) == True:\n",
    "                    navType.append(2)\n",
    "                else:\n",
    "                    navType.append(0)\n",
    "        except:\n",
    "                NumOfPage.append(0)\n",
    "                navType.append(0)\n",
    "                pass\n",
    "    \n",
    "    name_url = len(NumOfLinks)*[_url_]\n",
    "    if (NumOfLinks ==[] or NumOfLinks is None):\n",
    "        NumOfLinks = [0]\n",
    "    if (NumOfButton ==[] or NumOfButton is None):\n",
    "        NumOfButton = [0] \n",
    "    if (commonURL ==[] or commonURL is None):\n",
    "        commonURL = [0] \n",
    "    if (is_page == [] or is_page is None):\n",
    "        is_page = [0]    \n",
    "    if (NumOfPage == [] or NumOfPage is None):\n",
    "        NumOfPage = [0]\n",
    "    if (NumberOfValues == [] or NumberOfValues is None):\n",
    "        NumberOfValues = [0]\n",
    "    if (navType == [] or navType is None):\n",
    "        navType = [0]\n",
    "    #print(len(navType), len(name_url), len(NumOfButton), len(NumOfLinks), (commonURL), len(is_page), len(pageClass), len(NumOfPage))     \n",
    "    return name_url, NumOfButton, NumOfLinks, commonURL, NumberOfValues, is_page, NumOfPage, navType \n",
    "\n",
    "#print(pageListFunction(turl[4]))\n",
    "def get_class_data(searchQ) :\n",
    "        start_time= time.time()\n",
    "        name_url, NumOfButton, NumOfLinks, commonURL,NumberOfValues, is_page, NumOfPage, navType = pageListFunction(searchQ)\n",
    "        #checkBoxList = str(checkBoxList)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        #insideList = str(insideList)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        #filterClass = str(filterClass)[1:-1].replace(\",\",\"\").replace(\" \",\"\")        \n",
    "        temp = []\n",
    "        temp2 = []\n",
    "        for i in NumOfButton:\n",
    "            NumOfButton = i\n",
    "            temp.append([NumOfButton])        \n",
    "        \n",
    "        t_NumOfLinks= []\n",
    "        for m in NumOfLinks:\n",
    "            t_NumOfLinks.append(m)\n",
    "        NumOfLinks_arr2d = np.matrix(temp)\n",
    "        NumOfLinks_to_add = np.array(t_NumOfLinks)\n",
    "        output_NumOfLinks = np.column_stack((NumOfLinks_arr2d, NumOfLinks_to_add))\n",
    "        f_NumOfLinks = output_NumOfLinks.tolist()\n",
    "        \n",
    "        t_commonURL = []\n",
    "        for m in commonURL:\n",
    "            t_commonURL.append(m)\n",
    "        commonURL_arr2d = np.matrix(f_NumOfLinks)\n",
    "        commonURL_to_add = np.array(t_commonURL)\n",
    "        output_commonURL = np.column_stack((commonURL_arr2d, commonURL_to_add))\n",
    "        f_commonURL = output_commonURL.tolist()\n",
    "        \n",
    "        t_is_Page = []\n",
    "        for m in is_page:\n",
    "            t_is_Page.append(m)\n",
    "        is_Page_arr2d = np.matrix(f_commonURL)\n",
    "        is_Page_to_add = np.array(t_is_Page)\n",
    "        output_is_Page = np.column_stack((is_Page_arr2d, is_Page_to_add))\n",
    "        f_is_Page = output_is_Page.tolist()\n",
    "        \n",
    "        t_NumOfPage = []\n",
    "        for m in NumOfPage:\n",
    "            t_NumOfPage.append(m)\n",
    "        NumOfPage_arr2d = np.matrix(f_is_Page)\n",
    "        NumOfPage_to_add = np.array(t_NumOfPage)\n",
    "        output_NumOfPage = np.column_stack((NumOfPage_arr2d, NumOfPage_to_add))\n",
    "        f_NumOfPage = output_NumOfPage.tolist()\n",
    "        \n",
    "        t_NumberOfValues = []\n",
    "        for m in NumberOfValues:\n",
    "            t_NumberOfValues.append(m)\n",
    "        NumberOfValues_arr2d = np.matrix(f_NumOfPage)\n",
    "        NumberOfValues_to_add = np.array(t_NumberOfValues)\n",
    "        output_NumberOfValues = np.column_stack((NumberOfValues_arr2d, NumberOfValues_to_add))\n",
    "        f_NumberOfValues = output_NumberOfValues.tolist()\n",
    "        \n",
    "        t_navType = []\n",
    "        for m in navType:\n",
    "            t_navType.append(m)\n",
    "        navType_arr2d = np.matrix(f_NumberOfValues)\n",
    "        navType_to_add = np.array(t_navType)\n",
    "        output_navType = np.column_stack((navType_arr2d, navType_to_add))\n",
    "        f_navType = output_navType.tolist()\n",
    "        \n",
    "\n",
    "        \n",
    "        t_name= []\n",
    "        for m in name_url:\n",
    "            t_name.append(m)\n",
    "        a_name = np.matrix(f_navType)\n",
    "        column_name = np.array(name_url)\n",
    "        o_name = np.column_stack((a_name, column_name))\n",
    "        f_name= o_name.tolist()\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        hours, rem = divmod(end-start_time, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"\\nTime takes: {:0>2}:{:0>2}:{:05.2f} Seconds\\n\".format(int(hours),int(minutes),seconds))\n",
    "        return f_name\n",
    "    \n",
    "#get_class_data(turl[4])    \n",
    "def write_header():\n",
    "    list_of_header = [\"NumOfButton\", \"NumOfLinks\", \"commonURL\",\"is_page\", \"NumOfPage\", \"NumberOfValues\",\"navType\", \"name_url\"]\n",
    "    save_path = 'result/page/'\n",
    "    file_name = \"page_test_set__1.csv\"\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "\n",
    "    with open(completeName, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list_of_header)\n",
    "\n",
    "def write_CSV(tlist):\n",
    "    save_path = 'result/page/'\n",
    "    file_name = \"page_test_set__1.csv\"\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "    with open(completeName, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(tlist)\n",
    "    with open(completeName, \"r\", newline=\"\") as fr:\n",
    "        reader = csv.reader(fr)\n",
    "        lines= len(list(reader))\n",
    "        print(\"[\",lines,\"].\", \"form!\")\n",
    "\n",
    "def main():\n",
    "    write_header()\n",
    "    for i in range(0,100):\n",
    "            print(write_CSV(get_class_data(turl[i])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./result/page/page_test_set__1.csv\")\n",
    "\n",
    "for (gender), group in data.groupby(['name_url']):\n",
    "     group.to_csv(f'{gender}.csv', index=False)\n",
    "\n",
    "# for i in range(0,1):\n",
    "#     namecsv = turl[i]+\".csv\"\n",
    "# print(pd.read_csv(namecsv))\n",
    "# # print(pd.read_csv(\"/Users/mdjavedulferdous/Desktop/TiiS/Code/result/search/Ajio.html.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f782e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluation(Search_X_test,searchname,target_names,search_URL_name):\n",
    "    search_model = pickle.load(open(searchname, 'rb'))\n",
    "    y_score = search_model.predict(Search_X_test)\n",
    "    y_score_1 = search_model.predict_proba(Search_X_test)*100\n",
    "\n",
    "    for i in range(len(y_score_1)):\n",
    "        if y_score_1[i][1] >y_score_1[i][0]:\n",
    "            print([i],\"Positive Result\")\n",
    "            print(\"TRUE: \",y_score_1[i][1])\n",
    "            print(\"FALSE: \",y_score_1[i][0])\n",
    "            print(\"==========================\")\n",
    "        else:\n",
    "            print([i],\"Negative Result\")\n",
    "            print(\"TRUE: \",y_score_1[i][1])\n",
    "            print(\"FALSE: \",y_score_1[i][0])\n",
    "            print(\"==========================\")\n",
    "    df = pd.DataFrame(y_score_1) \n",
    "    return (df)\n",
    "\n",
    "\n",
    "page_path = '/Users/mdjavedulferdous/Desktop/TiiS/Code/result/page/model_csv/Cleartrip.html.csv'\n",
    "page_csv_df = pd.read_csv(page_path)\n",
    "page_URL_name = page_csv_df[[\"name_url\"]]\n",
    "page_X_test = page_csv_df[[\"NumOfButton\", \"NumOfLinks\", \"commonURL\",\"is_page\", \"NumOfPage\", \"NumberOfValues\",\"navType\"]]\n",
    "pagename = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/page_model.sav\"\n",
    "target_names = ['Non-page', 'page']\n",
    "page_URL_name = (page_URL_name[\"name_url\"][0].split('/')[-1]).split('.')[0]\n",
    "nameCSV = page_URL_name+'.csv'\n",
    "\n",
    "data1_import = evaluation(page_X_test,pagename,target_names,page_URL_name)\n",
    "data2_import = pd.read_csv(page_path)   \n",
    "data_merge = pd.concat([data1_import, data2_import],axis=1)\n",
    "\n",
    "data_merge.to_csv(nameCSV, index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a54bb",
   "metadata": {},
   "source": [
    "### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e903942d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sortFunc(_url_):\n",
    "    sort_inner, sortClass,option_tag_attribute_value, sort_attribute,t_count  = ([] for i in range(5)) \n",
    "    myFile=open(_url_,'r',encoding=\"latin-1\")\n",
    "    \n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    \n",
    "    for tests in soup.findAll('select'):\n",
    "            t_Price = len(tests.findAll(text=re.compile(\".*Price.*\"))) \n",
    "            t_Most = len(tests.findAll(text=re.compile(\".*Most recent.*\")))\n",
    "            t_New = len(tests.findAll(text=re.compile(\".*New.*\")))\n",
    "            t_Best = len(tests.findAll(text=re.compile(\".*Best Match.*\")))  \n",
    "            t_Highest = len(tests.findAll(text=re.compile(\".*Highest*\"))) \n",
    "            t_Ratings = len(tests.findAll(text=re.compile(\".*Ratings.*\"))) \n",
    "            t_Distance = len(tests.findAll(text=re.compile(\".*Distance*\"))) \n",
    "            t_Time = len(tests.findAll(text=re.compile(\".*Time*\"))) \n",
    "            t_Relevance = len(tests.findAll(text=re.compile(\".*Relevance*\"))) \n",
    "            t_Featured = len(tests.findAll(text=re.compile(\".*Featured.*\"))) \n",
    "            t_Recommended = len(tests.findAll(text=re.compile(\".*Recommended.*\")))\n",
    "            t_count.append((int(t_Price)+int(t_Most)+int(t_New)+int(t_Best)+int(t_Highest)+int(t_Ratings)+int(t_Distance)+int(t_Time)+int(t_Relevance)+int(t_Featured)+int(t_Recommended)))\n",
    "\n",
    "            if \" \" == tests.text:\n",
    "                sort_inner.append(0)             \n",
    "            else:\n",
    "                #print(tests.text)\n",
    "                if (t_Price or t_Most or t_New or t_Best or  t_Highest or t_Ratings or t_Distance or t_Time or t_Relevance or t_Featured or t_Recommended)!=0:\n",
    "                    #print(tests(text=lambda t: \"sort:\" in t))\n",
    "                    sort_inner.append(1)\n",
    "                else:\n",
    "                    sort_inner.append(0)             \n",
    "            if('sort' in list(tests.attrs.values())):\n",
    "                #print(tests.attrs.values())\n",
    "                sort_attribute.append(1)\n",
    "            else:\n",
    "                sort_attribute.append(0) \n",
    "            optionTag = tests.findAll(\"option\")\n",
    "            option_tag_attribute_value.append(len(optionTag))\n",
    " \n",
    "    for tests in soup.findAll('ul'):\n",
    "            t_Price = len(tests.findAll(text=re.compile(\".*Price.*\"))) \n",
    "            t_Most = len(tests.findAll(text=re.compile(\".*Most recent.*\")))\n",
    "            t_New = len(tests.findAll(text=re.compile(\".*New.*\")))\n",
    "            t_Best = len(tests.findAll(text=re.compile(\".*Best Match.*\")))  \n",
    "            t_Highest = len(tests.findAll(text=re.compile(\".*Highest*\"))) \n",
    "            t_Ratings = len(tests.findAll(text=re.compile(\".*Ratings.*\"))) \n",
    "            t_Distance = len(tests.findAll(text=re.compile(\".*Distance*\"))) \n",
    "            t_Time = len(tests.findAll(text=re.compile(\".*Time*\"))) \n",
    "            t_Relevance = len(tests.findAll(text=re.compile(\".*Relevance*\"))) \n",
    "            t_Featured = len(tests.findAll(text=re.compile(\".*Featured.*\"))) \n",
    "            t_Recommended = len(tests.findAll(text=re.compile(\".*Recommended.*\")))\n",
    "            t_count.append((int(t_Price)+int(t_Most)+int(t_New)+int(t_Best)+int(t_Highest)+int(t_Ratings)+int(t_Distance)+int(t_Time)+int(t_Relevance)+int(t_Featured)+int(t_Recommended)))\n",
    "      \n",
    "            if \" \" == tests.text:\n",
    "                sort_inner.append(0)             \n",
    "            else:\n",
    "                #print(tests.text)\n",
    "                if (t_Price or t_Most or t_New or t_Best or  t_Highest or t_Ratings or t_Distance or t_Time or t_Relevance or t_Featured or t_Recommended)!=0:\n",
    "                    #print(tests(text=lambda t: \"sort:\" in t))\n",
    "                    sort_inner.append(1)\n",
    "                else:\n",
    "                    sort_inner.append(0)             \n",
    "            if('sort' in list(tests.attrs.values())):\n",
    "                #print(tests.attrs.values())\n",
    "                sort_attribute.append(1)\n",
    "            else:\n",
    "                sort_attribute.append(0) \n",
    "            optionTag = tests.findAll(\"li\")\n",
    "            option_tag_attribute_value.append(len(optionTag))\n",
    "    else:\n",
    "        sort_inner.append(0)\n",
    "        option_tag_attribute_value.append(0)\n",
    "        sort_attribute.append(0)\n",
    "        t_count.append(0)\n",
    "    if (sort_inner ==[]):\n",
    "        sort_inner = [0]\n",
    "    if (sort_attribute ==[]):\n",
    "        sort_attribute = [0]\n",
    "    if (option_tag_attribute_value ==[]):\n",
    "        option_tag_attribute_value = [0]\n",
    "\n",
    "    if (t_count ==[]):\n",
    "        t_count = [0]    \n",
    "    temp = len(sort_inner)*[_url_]\n",
    "    \n",
    "    return temp, sort_inner,  option_tag_attribute_value, sort_attribute,t_count\n",
    "#print(sortFunc(turl[5]))\n",
    "# for i in range(10):\n",
    "#      print([i],sortFunc(turl[i]))\n",
    "def get_class_data(searchQ) :\n",
    "        name_url, sort_inner, option_tag_attribute_value, sort_attribute, textCount = sortFunc(searchQ)\n",
    "        sort_inner = str(sort_inner)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        sort_attribute = str(sort_attribute)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        #option_tag_attribute_value = str(option_tag_attribute_value)[1:-1]\n",
    "        temp = []\n",
    "        temp2 = []\n",
    "        for i in sort_inner:\n",
    "            sort_inner = i\n",
    "            temp.append([sort_inner])        \n",
    "        \n",
    "        t_sort_attribute = []\n",
    "        for m in sort_attribute:\n",
    "            t_sort_attribute.append(m)\n",
    "        arr2d = np.matrix(temp)\n",
    "        column_to_add = np.array(t_sort_attribute)\n",
    "        output = np.column_stack((arr2d, column_to_add))\n",
    "        f_sort_attribute = output.tolist()\n",
    "        \n",
    "        t_option = []\n",
    "        for m in option_tag_attribute_value:\n",
    "            t_option.append(m)\n",
    "        option_arr2d = np.matrix(f_sort_attribute)\n",
    "        option_to_add = np.array(t_option)\n",
    "        output_option = np.column_stack((option_arr2d, option_to_add))\n",
    "        f_option = output_option.tolist()\n",
    "    \n",
    "        t_textCount = []\n",
    "        for m in textCount:\n",
    "            t_textCount.append(m)\n",
    "        textCount_arr2d = np.matrix(f_option)\n",
    "        textCount_add = np.array(t_textCount)\n",
    "        textCount_output = np.column_stack((textCount_arr2d, textCount_add))\n",
    "        f_textCount = textCount_output.tolist()\n",
    "        \n",
    "        t_name= []\n",
    "        for m in name_url:\n",
    "            t_name.append(m)\n",
    "        a_name = np.matrix(f_textCount)\n",
    "        column_name = np.array(t_name)\n",
    "        o_name = np.column_stack((a_name, column_name))\n",
    "        f_name= o_name.tolist()\n",
    "\n",
    "        return f_name\n",
    "get_class_data(turl[3])\n",
    "\n",
    "# def write_header():\n",
    "#     list_of_header = [\"sort_inner\", \"sort_attribute\", \"option_tag_attribute_value\",\"textCount\",\"name_url\"]\n",
    "#     save_path = 'result/sort/'\n",
    "#     file_name = \"sort_test_set__1.csv\"\n",
    "#     completeName = os.path.join(save_path, file_name)\n",
    "\n",
    "#     with open(completeName, \"a\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow(list_of_header)\n",
    "\n",
    "# def write_CSV(tlist):\n",
    "#     save_path = 'result/sort/'\n",
    "#     file_name = \"sort_test_set__1.csv\"\n",
    "#     completeName = os.path.join(save_path, file_name)\n",
    "#     with open(completeName, \"a\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerows(tlist)\n",
    "#     with open(completeName, \"r\", newline=\"\") as fr:\n",
    "#         reader = csv.reader(fr)\n",
    "#         lines= len(list(reader))\n",
    "#         print(\"[\",lines,\"].\", \"form!\")\n",
    "\n",
    "# def main():\n",
    "#     write_header()\n",
    "#     for i in range(0,100):\n",
    "#             print(write_CSV(get_class_data(turl[i])))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./result/sort/sort_test_set__1.csv\")\n",
    "print()\n",
    "for (gender), group in data.groupby(['name_url']):\n",
    "     group.to_csv(f'{gender}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def div_extract_div(_url_):\n",
    "#     myFile=open(_url_,'r',encoding=\"latin-1\")\n",
    "#     soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "#     a_tag, a_tag_p,a_tag_gp, sort, spanTag, sTagp, sTagGP, buttonTag, buttonTagP, buttonTagGP,inputTag, inputTagP, inputTagGP, divTag, divTagP, divTagGP = ([] for i in range(16))\n",
    "    \n",
    "#     for elem in soup.findAll('a'):\n",
    "#         a_tag.append(elem)\n",
    "        \n",
    "#     for sTag in soup.findAll('span'):\n",
    "#         spanTag.append(sTag)\n",
    "        \n",
    "#     for bTag in soup.findAll('button'):\n",
    "#         buttonTag.append(bTag)     \n",
    "        \n",
    "#     for iTag in soup.findAll('input'):\n",
    "#         inputTag.append(iTag)\n",
    "    \n",
    "#     for dTag in soup.findAll('div'):\n",
    "#         divTag.append(dTag)    \n",
    "    \n",
    "#     a_tag_p, a_tag_gp =  ancester_search(a_tag)\n",
    "#     sTagp, sTagGP =  ancester_search(spanTag)\n",
    "#     buttonTagP, buttonTagGP =  ancester_search(buttonTag)\n",
    "#     inputTagP, inputTagGP =  ancester_search(inputTag)\n",
    "#     divTagP, divTagGP =  ancester_search(divTag)\n",
    "\n",
    "#     sort_innerA, sortA, sort_attributeA, option_tag_attribute_valueA, t_countA = data_attribute_a(a_tag_p, \"a\")\n",
    "#     sort_innerB, sortB, sort_attributeB, option_tag_attribute_valueB, t_countB = data_attribute_a(a_tag_gp, \"a\")\n",
    "#     sort_innerA1, sortA1, sort_attributeA1, option_tag_attribute_valueA1, t_countA1 = data_attribute_a(sTagp, \"span\")\n",
    "#     sort_innerB1, sortB1, sort_attributeB1, option_tag_attribute_valueB1, t_countB1 = data_attribute_a(sTagGP, \"span\")    \n",
    "#     sort_innerA2, sortA2, sort_attributeA2, option_tag_attribute_valueA2, t_countA2 = data_attribute_a(buttonTagP, \"button\")\n",
    "#     sort_innerB2, sortB2, sort_attributeB2, option_tag_attribute_valueB2, t_countB2 = data_attribute_a(buttonTagGP, \"button\")    \n",
    "    \n",
    "#     sort_innerA3, sortA3, sort_attributeA3, option_tag_attribute_valueA3, t_countA3 = data_attribute_a(inputTagP, \"input\")\n",
    "#     sort_innerB3, sortB3, sort_attributeB3, option_tag_attribute_valueB3, t_countB3 = data_attribute_a(inputTagGP, \"input\")    \n",
    "#     #temp = len(sort_innerB3)*[_url_]\n",
    "\n",
    "#     #return temp, sort_innerB3, sortB3, sort_attributeB3, option_tag_attribute_valueB3, t_countB3\n",
    "    \n",
    "#     sort_innerA4, sortA4, sort_attributeA4, option_tag_attribute_valueA4, t_countA4 = data_attribute_a(divTagP, \"div\")    \n",
    "#     sort_innerB4, sortB4, sort_attributeB4, option_tag_attribute_valueB4, t_countB4 = data_attribute_a(divTagGP, \"div\")    \n",
    "    \n",
    "#     if ((sortA).count(1)) ==1 or  ((sortB).count(1)) ==1:\n",
    "#         if((sortA).count(1)) > ((sortB).count(1)):\n",
    "#             temp = len(sort_innerA)*[_url_]\n",
    "#             return temp, sort_innerA, sortA, sort_attributeA, option_tag_attribute_valueA, t_countA\n",
    "#         elif((sortA).count(1)) < ((sortB).count(1)):\n",
    "#             temp = len(sort_innerB)*[_url_]\n",
    "#             return temp,  sort_innerB, sortB, sort_attributeB, option_tag_attribute_valueB, t_countB  \n",
    "#         else:\n",
    "#             temp = len(sort_innerA)*[_url_]\n",
    "#             return temp, sort_innerA, sortA, sort_attributeA, option_tag_attribute_valueA, t_countA\n",
    "    \n",
    "#     elif ((sortA1).count(1))==1 or ((sortB1).count(1))== 1:\n",
    "#         if((sortA1).count(1)) >((sortB1).count(1)):\n",
    "#             temp = len(sort_innerA1)*[_url_]\n",
    "#             return temp,  sort_innerA1, sortA1, sort_attributeA1, option_tag_attribute_valueA1, t_countA1\n",
    "#         elif ((sortA1).count(1)) < ((sortB1).count(1)):\n",
    "#             temp = len(sort_innerB1)*[_url_]\n",
    "#             return temp,  sort_innerB1, sortB1, sort_attributeB1, option_tag_attribute_valueB1, t_countB1\n",
    "#         else:\n",
    "#             temp = len(sort_innerA1)*[_url_]\n",
    "#             return temp,  sort_innerA1, sortA1, sort_attributeA1, option_tag_attribute_valueA1, t_countA1\n",
    "        \n",
    "#     elif sortA1.count(1) == 1:\n",
    "#         if sortB1.count(1) ==1:\n",
    "#             temp = len(sort_innerA1)*[_url_]\n",
    "#             print(sortA1.count(1), len(sortA1))\n",
    "\n",
    "#             print(sortB1.count(1), len(sortB1))\n",
    "\n",
    "#             return temp,  sort_innerA1, sortA1, sort_attributeA1, option_tag_attribute_valueA1, t_countA1\n",
    "    \n",
    "#     elif ((sortA2).count(1))==1 or ((sortB2).count(1)) ==1:\n",
    "#         if ((sortA2).count(1)) > ((sortB2).count(1)):\n",
    "#             temp = len(sort_innerA2)*[_url_]\n",
    "#             return temp,  sort_innerA2, sortA2, sort_attributeA2, option_tag_attribute_valueA2, t_countA2\n",
    "#         elif ((sortA2).count(1)) < ((sortB2).count(1)):\n",
    "#             temp = len(sort_innerB2)*[_url_]\n",
    "#             return temp,  sort_innerB2, sortB2, sort_attributeB2, option_tag_attribute_valueB2, t_countB2\n",
    "#         else:\n",
    "#             temp = len(sort_innerA2)*[_url_]\n",
    "#             return temp,  sort_innerA2, sortA2, sort_attributeA2, option_tag_attribute_valueA2, t_countA2\n",
    "    \n",
    "#     elif ((sortA3).count(1)) ==1 or  ((sortB3).count(1))==1:\n",
    "#         if((sortA3).count(1)) > ((sortB3).count(1)):\n",
    "#             temp = len(sort_innerA3)*[_url_]\n",
    "#             return temp,  sort_innerA3, sortA3, sort_attributeA3, option_tag_attribute_valueA3, t_countA3\n",
    "#         elif ((sortA3).count(1)) < ((sortB3).count(1)):\n",
    "#             temp = len(sort_innerB3)*[_url_]\n",
    "#             return temp, sort_innerB3, sortB3, sort_attributeB3, option_tag_attribute_valueB3, t_countB3\n",
    "#         else:\n",
    "#             temp = len(sort_innerA3)*[_url_]\n",
    "#             return temp,  sort_innerA3, sortA3, sort_attributeA3, option_tag_attribute_valueA3, t_countA3\n",
    "    \n",
    "#     elif ((sortA4).count(1)) ==1 or ((sortB4).count(1))==1:\n",
    "#         if((sortA4).count(1)) > ((sortB4).count(1)):\n",
    "#             temp = len(sort_innerA4)*[_url_]\n",
    "#             return temp,  sort_innerA4, sortA4, sort_attributeA4, option_tag_attribute_valueA4, t_countA4\n",
    "#         elif ((sortA4).count(1)) < ((sortB4).count(1)):\n",
    "#             temp = len(sort_innerB4)*[_url_]\n",
    "#             return temp,  sort_innerB4, sortB4, sort_attributeB4, option_tag_attribute_valueB4, t_countB4\n",
    "#         else:\n",
    "#             temp =len(sort_innerB4)*[_url_]\n",
    "#             return temp,  sort_innerA4, sortA4, sort_attributeA4, option_tag_attribute_valueA4, t_countA4\n",
    "    \n",
    "#     else:\n",
    "#         return _url_,[0],[1],[0],[0],[0]\n",
    "    \n",
    "# def ancester_search(_list_):\n",
    "#     a_tag_p,a_tag_gp = [], []\n",
    "#     for i in range(0, len(_list_)):\n",
    "#         a_tag_p.append(_list_[i].find_previous())\n",
    "#         a_tag_gp.append(_list_[i].find_previous().find_previous())\n",
    "#     return a_tag_p,a_tag_gp\n",
    "        \n",
    "# def data_attribute_a(_list_, tag):\n",
    "#     a = 0\n",
    "#     sort_inner, sort,sort_attribute,option_tag_attribute_value, temp, t_count = ([] for i in range(6)) \n",
    "#     for i in range(0, len(_list_)):\n",
    "#         t_Price = len(_list_[i].findAll(text=re.compile(\".*Price.*\"))) \n",
    "#         t_Most = len(_list_[i].findAll(text=re.compile(\".*Most recent.*\")))\n",
    "#         t_New = len(_list_[i].findAll(text=re.compile(\".*New.*\")))\n",
    "#         t_Best = len(_list_[i].findAll(text=re.compile(\".*Best Match.*\")))  \n",
    "#         t_Highest = len(_list_[i].findAll(text=re.compile(\".*Highest*\"))) \n",
    "#         t_Ratings = len(_list_[i].findAll(text=re.compile(\".*Ratings.*\"))) \n",
    "#         t_Distance = len(_list_[i].findAll(text=re.compile(\".*Distance*\"))) \n",
    "#         t_Time = len(_list_[i].findAll(text=re.compile(\".*Time*\"))) \n",
    "#         t_Relevance = len(_list_[i].findAll(text=re.compile(\".*Relevance*\"))) \n",
    "#         t_Featured = len(_list_[i].findAll(text=re.compile(\".*Featured.*\"))) \n",
    "#         t_Recommended = len(_list_[i].findAll(text=re.compile(\".*Recommended.*\")))\n",
    "#         t_count.append((int(t_Price)+int(t_Most)+int(t_New)+int(t_Best)+int(t_Highest)+int(t_Ratings)+int(t_Distance)+int(t_Time)+int(t_Relevance)+int(t_Featured)+int(t_Recommended)))\n",
    "#         if \"data-attribute\" in list(_list_[i].attrs.keys()):\n",
    "#             if \"sort\" in list(_list_[i].attrs.values()):\n",
    "#                 sort.append(1)\n",
    "#                 #print(_list_[i].name)\n",
    "#             else:\n",
    "#                 sort.append(0)\n",
    "#         else:\n",
    "#             sort.append(0)\n",
    "#         if \" \" == _list_[i].text:\n",
    "#                 sort_inner.append(0)             \n",
    "#         else:\n",
    "#                 if (t_Price or t_Most or t_New or t_Best or  t_Highest or t_Ratings or t_Distance or t_Time or t_Relevance or t_Featured or t_Recommended)!=0:\n",
    "#                     sort_inner.append(1)\n",
    "#                 else:\n",
    "#                     sort_inner.append(0)\n",
    "#         if('sort' in list(_list_[i].attrs.values())):\n",
    "#                 sort_attribute.append(1)\n",
    "#         else:\n",
    "#                 sort_attribute.append(0) \n",
    "#         optionTag = len(_list_[i].find_all(tag))\n",
    "#         option_tag_attribute_value.append(optionTag)\n",
    "#     if (sort ==[]):\n",
    "#         sort = [0]\n",
    "#     if (sort_inner ==[]):\n",
    "#         sort_inner = [0]\n",
    "#     if (sort_attribute ==[]):\n",
    "#         sort_attribute = [0]\n",
    "#     if (option_tag_attribute_value ==[]):\n",
    "#         option_tag_attribute_value = [0]\n",
    "#     if (t_count ==[]):\n",
    "#         t_count = [0]  \n",
    "#     print(sort.count(1), len(sort))\n",
    "#     if(sort.count(1) == 1):\n",
    "#         print(tag)\n",
    "#     #print(len(sort), len(sort_inner), len(sort_attribute), len(option_tag_attribute_value))\n",
    "#     return sort_inner, sort, sort_attribute, option_tag_attribute_value, t_count\n",
    "# def get_class_data(searchQ) :\n",
    "#         start_time= time.time()\n",
    "#         name_url, sort_inner, sortClass, sort_attribute, option_tag_attribute_value,  textCount = div_extract_div(searchQ)\n",
    "#         #name_url, sort_inner, sortClass, sort_attribute, option_tag_attribute_value,  textCount = testing()\n",
    "#         sort_inner = str(sort_inner)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "#         sort_attribute = str(sort_attribute)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "#         #option_tag_attribute_value = str(option_tag_attribute_value)[1:-1]\n",
    "#         sortClass = str(sortClass)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "#         temp = []\n",
    "#         temp2 = []\n",
    "#         for i in sort_inner:\n",
    "#             sort_inner = i\n",
    "#             temp.append([sort_inner])        \n",
    "\n",
    "#             t_sort_attribute = []\n",
    "#         for m in sort_attribute:\n",
    "#             t_sort_attribute.append(m)\n",
    "#         arr2d = np.matrix(temp)\n",
    "#         column_to_add = np.array(t_sort_attribute)\n",
    "#         output = np.column_stack((arr2d, column_to_add))\n",
    "#         f_sort_attribute = output.tolist()\n",
    "        \n",
    "#         t_option = []\n",
    "#         for m in option_tag_attribute_value:\n",
    "#             t_option.append(m)\n",
    "#         option_arr2d = np.matrix(f_sort_attribute)\n",
    "#         option_to_add = np.array(t_option)\n",
    "#         output_option = np.column_stack((option_arr2d, option_to_add))\n",
    "#         f_option = output_option.tolist()\n",
    "        \n",
    "#         t_sortClass = []\n",
    "#         for m in sortClass:\n",
    "#             t_sortClass.append(m)\n",
    "#         sortClass_arr2d = np.matrix(f_option)\n",
    "#         sortClass_add = np.array(t_sortClass)\n",
    "#         sortClass_output = np.column_stack((sortClass_arr2d, sortClass_add))\n",
    "#         f_sortClass = sortClass_output.tolist()\n",
    "        \n",
    "#         t_textCount = []\n",
    "#         for m in textCount:\n",
    "#             t_textCount.append(m)\n",
    "#         textCount_arr2d = np.matrix(f_sortClass)\n",
    "#         textCount_add = np.array(t_textCount)\n",
    "#         textCount_output = np.column_stack((textCount_arr2d, textCount_add))\n",
    "#         f_textCount = textCount_output.tolist()\n",
    "#         '''\n",
    "#         t_name= []\n",
    "#         if(len(name_url)==1):\n",
    "#             t_name.append(name_url)\n",
    "#         else:\n",
    "#             for m in name_url:\n",
    "#                 t_name.append(m)\n",
    "#         '''\n",
    "#         a_name = np.matrix(f_textCount)\n",
    "#         column_name = np.array(name_url)\n",
    "#         o_name = np.column_stack((a_name, column_name))\n",
    "#         f_name= o_name.tolist()\n",
    "#         end = time.time()\n",
    "#         hours, rem = divmod(end-start_time, 3600)\n",
    "#         minutes, seconds = divmod(rem, 60)\n",
    "#         print(\"\\nTime takes: {:0>2}:{:0>2}:{:05.2f} Seconds\\n\".format(int(hours),int(minutes),seconds))\n",
    "#         return f_name\n",
    "# def write_header():\n",
    "#     list_of_header = [\"sort_inner\", \"sort_attribute\", \"option_tag_attribute_value\",\"sortClass\",\"textCount\",\"name_url\"]\n",
    "#     save_path = 'result/'\n",
    "#     file_name = \"sort_test_set_1.csv\"\n",
    "#     completeName = os.path.join(save_path, file_name)\n",
    "\n",
    "#     with open(completeName, \"a\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow(list_of_header)\n",
    "\n",
    "# def write_CSV(tlist):\n",
    "#     save_path = 'result/'\n",
    "#     file_name = \"sort_test_set_1.csv\"\n",
    "#     completeName = os.path.join(save_path, file_name)\n",
    "#     with open(completeName, \"a\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerows(tlist)\n",
    "#     with open(completeName, \"r\", newline=\"\") as fr:\n",
    "#         reader = csv.reader(fr)\n",
    "#         lines= len(list(reader))\n",
    "#         print(\"[\",lines,\"].\", \"form!\")\n",
    "\n",
    "# def main():\n",
    "#     write_header()\n",
    "#     for i in range(0,100):\n",
    "#             print(write_CSV(get_class_data(turl[i])))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a7606",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2238f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def filterFunc(_url_):\n",
    "    sort_inner, sortClass,option_tag_attribute_value, sort_attribute,t_count  = ([] for i in range(5)) \n",
    "    \n",
    "    myFile=open(_url_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    \n",
    "    divFilter, ulFilter, liFilter, formFilter, sectionFilter, fieldsetFilter, dtFilter,\\\n",
    "    buttonFilter,articleFilter,dlFilter,desktopfacetFilter = \\\n",
    "    soup.findAll('div'),soup.findAll('ul'),\\\n",
    "    soup.findAll('li'), soup.findAll('form'),\\\n",
    "    soup.findAll('section'), soup.findAll('fieldset'),\\\n",
    "    soup.findAll('dt',{\"data-attribute\":\"filter\"}),soup.findAll('button'),\\\n",
    "    soup.findAll('article'), soup.findAll('dl'),\\\n",
    "    soup.findAll('desktop-facet')\n",
    "    try:\n",
    "        if divFilter != []:    \n",
    "            return filter_Extract(soup, 'div',_url_)\n",
    "        if liFilter != []:    \n",
    "            return filter_Extract(soup, 'li',_url_)\n",
    "        if buttonFilter != []:    \n",
    "            return filter_Extract(soup, 'button',_url_)\n",
    "        if dlFilter != []:    \n",
    "            return filter_Extract(soup, 'dl',_url_)\n",
    "        if desktopfacetFilter != []:    \n",
    "            return filter_Extract(soup, 'desktop-facet',_url_)\n",
    "        if articleFilter != []:    \n",
    "            return filter_Extract(soup, 'article',_url_)\n",
    "        if dtFilter != []:    \n",
    "            return filter_Extract(soup, 'dt',_url_)\n",
    "        if fieldsetFilter != []:    \n",
    "            return filter_Extract(soup, 'fieldset',_url_)\n",
    "        if sectionFilter != []:    \n",
    "            return filter_Extract(soup, 'section',_url_)\n",
    "        if formFilter != []:    \n",
    "            return filter_Extract(soup, 'form',_url_)\n",
    "        if ulFilter != []:    \n",
    "            return filter_Extract(soup, 'ul',_url_)\n",
    "        else:\n",
    "            return _url_,\"0\",\"0\",\"0\",\"0\",\"0\"\n",
    "    except:\n",
    "        pass\n",
    "def filter_Extract(soup, tag,_url_):\n",
    "    fCount  = 0\n",
    "    filterClass, checkBoxList,insideList,outerList, NumberOfFilter, NumberOfLink, temp2,\\\n",
    "    NumberOfInput, URL_List, button_List = [], [], [], [], [], [], [], [], [], []\n",
    "    \n",
    "    for ele in soup.findAll(tag):\n",
    "                        #======================= filterClass =======================\n",
    "                        fCount +=1\n",
    "                        ch1 = 0\n",
    "                        #======================= Sub-Category ======================= \n",
    "                        child = 0\n",
    "                        for scat in ele.findAll('label'):\n",
    "                            child +=1\n",
    "                            #print(scat.text)\n",
    "                        insideList.append(child)\n",
    "                        #========================= CheckBox =========================             \n",
    "                        for chk in ele.findAll('input', {\"type\": \"checkbox\"}):\n",
    "                            pass\n",
    "                            ch1 += 1\n",
    "                        if ch1 !=0 and ch1 !=1:\n",
    "                            pass\n",
    "                            #print(ch1)\n",
    "                            checkBoxList.append(1)\n",
    "                        elif ch1 is None:\n",
    "                            pass\n",
    "                            #print(\"Empty\")\n",
    "                        else:\n",
    "                            checkBoxList.append(0)\n",
    "                      \n",
    "                        #=================== Number of link used =================== \n",
    "                        valueCounter =0\n",
    "                        for link in ele.findAll('a'):\n",
    "                            valueCounter += 1\n",
    "                        NumberOfLink.append(valueCounter)\n",
    "                        #=================== Number of inputbox =================== \n",
    "                        inputCounter = 0\n",
    "                        for link in ele.findAll('input'):\n",
    "                            inputCounter += 1\n",
    "                        NumberOfInput.append(inputCounter)\n",
    "                        #=================== Number of URL List =================== \n",
    "                        linkCount = 0\n",
    "                        for alink in ele.findAll('a', href=True):\n",
    "                            linkCount += 1\n",
    "                        URL_List.append(1)\n",
    "                        #=================== Number of button List =================== \n",
    "                        buttonCount = 0\n",
    "                        for btn in ele.findAll('button'):\n",
    "                            buttonCount += 1\n",
    "                        if buttonCount!=1:\n",
    "                            button_List.append(1)\n",
    "                        else:\n",
    "                            button_List.append(0)\n",
    "   \n",
    "    name_url = len(NumberOfLink)*[_url_]\n",
    "    if (NumberOfLink ==[] or NumberOfLink is None):\n",
    "        NumberOfLink = [0]\n",
    "    if (NumberOfInput ==[] or NumberOfInput is None):\n",
    "        NumberOfInput = [0] \n",
    "    if (checkBoxList ==[] or checkBoxList is None):\n",
    "        checkBoxList = [0]\n",
    "    if (URL_List ==[] or URL_List is None):\n",
    "        URL_List = [0]\n",
    "    if (button_List ==[] or button_List is None):\n",
    "        button_List = [0]\n",
    "\n",
    "    return name_url, checkBoxList, NumberOfLink, NumberOfInput, URL_List, button_List\n",
    "#print(filterFunc(turl[7]))\n",
    "def get_class_data(searchQ) :\n",
    "        start_time= time.time()\n",
    "        name_url, checkBoxList, NumberOfLink, NumberOfInput, URL_List, button_List = filterFunc(searchQ)\n",
    "        #checkBoxList = str(checkBoxList)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        #insideList = str(insideList)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "        #filterClass = str(filterClass)[1:-1].replace(\",\",\"\").replace(\" \",\"\")\n",
    "\n",
    "        \n",
    "        temp = []\n",
    "        temp2 = []\n",
    "        for i in checkBoxList:\n",
    "            checkBoxList = i\n",
    "            temp.append([checkBoxList])        \n",
    "        \n",
    "        t_NumberOfLink= []\n",
    "        for m in NumberOfLink:\n",
    "            t_NumberOfLink.append(m)\n",
    "        NumOfLinks_arr2d = np.matrix(temp)\n",
    "        NumOfLinks_to_add = np.array(t_NumberOfLink)\n",
    "        output_NumOfLinks = np.column_stack((NumOfLinks_arr2d, NumOfLinks_to_add))\n",
    "        f_NumOfLinks = output_NumOfLinks.tolist()\n",
    "        \n",
    "        t_NumberOfInput = []\n",
    "        for m in NumberOfInput:\n",
    "            t_NumberOfInput.append(m)\n",
    "        NumberOfInput_arr2d = np.matrix(f_NumOfLinks)\n",
    "        NumberOfInput_to_add = np.array(t_NumberOfInput)\n",
    "        output_NumberOfInput = np.column_stack((NumberOfInput_arr2d, NumberOfInput_to_add))\n",
    "        f_NumberOfInput = output_NumberOfInput.tolist()\n",
    "        \n",
    "        t_URL_List = []\n",
    "        for m in URL_List:\n",
    "            t_URL_List.append(m)\n",
    "        URL_List_arr2d = np.matrix(f_NumberOfInput)\n",
    "        URL_List_to_add = np.array(t_URL_List)\n",
    "        output_URL_List = np.column_stack((URL_List_arr2d, URL_List_to_add))\n",
    "        f_URL_List= output_URL_List.tolist()\n",
    "        \n",
    "        t_button_List = []\n",
    "        for m in button_List:\n",
    "            t_button_List.append(m)\n",
    "        button_List_arr2d = np.matrix(f_URL_List)\n",
    "        button_List_to_add = np.array(t_button_List)\n",
    "        output_button_List = np.column_stack((button_List_arr2d, button_List_to_add))\n",
    "        f_button_List= output_button_List.tolist()\n",
    "        \n",
    "        \n",
    "        t_name= []\n",
    "        for m in name_url:\n",
    "            t_name.append(m)\n",
    "        a_name = np.matrix(f_button_List)\n",
    "        column_name = np.array(name_url)\n",
    "        o_name = np.column_stack((a_name, column_name))\n",
    "        f_name= o_name.tolist()\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        hours, rem = divmod(end-start_time, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"\\nTime takes: {:0>2}:{:0>2}:{:05.2f} Seconds\\n\".format(int(hours),int(minutes),seconds))\n",
    "        return f_name\n",
    "    \n",
    "\n",
    "#get_class_data(turl[32])\n",
    "def write_header():\n",
    "    list_of_header = [\"checkBoxList\", \"NumberOfLink\",\"NumberOfInput\", \"URL_List\", \"button_List\", \"name_url\"]\n",
    "    save_path = 'result/filter/'\n",
    "    file_name = \"filter_test_set_1.csv\"\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "\n",
    "    with open(completeName, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list_of_header)\n",
    "\n",
    "def write_CSV(tlist):\n",
    "    save_path = 'result/filter/'\n",
    "    file_name = \"filter_test_set_1.csv\"\n",
    "    completeName = os.path.join(save_path, file_name)\n",
    "    with open(completeName, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(tlist)\n",
    "    with open(completeName, \"r\", newline=\"\") as fr:\n",
    "        reader = csv.reader(fr)\n",
    "        lines= len(list(reader))\n",
    "        print(\"[\",lines,\"].\", \"form!\")\n",
    "\n",
    "def main():\n",
    "    write_header()\n",
    "    for i in range(0,100):\n",
    "            print(write_CSV(get_class_data(turl[i])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18400805",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./result/filter/filter_test_set_1.csv\")\n",
    "\n",
    "for (gender), group in data.groupby(['name_url']):\n",
    "     group.to_csv(f'{gender}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af70806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(Search_X_test,searchname,target_names,search_URL_name):\n",
    "    p_class_0, p_class_1, r_class_0, r_class_1 = [], [], [], []\n",
    "    #searchname = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/search_model.sav\"\n",
    "    search_model = pickle.load(open(searchname, 'rb'))\n",
    "    y_score = search_model.predict(Search_X_test)\n",
    "    y_score_1 = search_model.predict_proba(Search_X_test)*100\n",
    "    #print(y_score_1)\n",
    "    for i in range(len(y_score_1)):\n",
    "        if y_score_1[i][1] >y_score_1[i][0]:\n",
    "            print([i],\"Positive Result\")\n",
    "            print(\"TRUE: \",y_score_1[i][1])\n",
    "            print(\"FALSE: \",y_score_1[i][0])\n",
    "            print(\"==========================\")\n",
    "        else:\n",
    "            print([i],\"Negative Result\")\n",
    "            print(\"TRUE: \",y_score_1[i][1])\n",
    "            print(\"FALSE: \",y_score_1[i][0])\n",
    "            print(\"==========================\")\n",
    "    df = pd.DataFrame(y_score_1) \n",
    "    return (df)\n",
    "#     print(\"Probability: \",search_model.predict_proba(Search_X_test)*100)\n",
    "#     print(\"After sigmoid layer: \", search_model.predict(Search_X_test))\n",
    "#     classes = search_model.classes_\n",
    "#     probas = search_model.predict_proba(Search_X_test)\n",
    "#     for class_name, proba in zip(classes, probas):\n",
    "#         print(f\"{class_name}: {proba}\")\n",
    "# #     classes = target_names\n",
    "#     y_pred_prob = search_model.predict_proba(Search_X_test)\n",
    "#     ix = y_pred_prob.argmax(1).item()\n",
    "    \n",
    "#     print(f'predicted class = {classes[ix]} and confidence = {y_pred_prob[0,ix]:.2%}')\n",
    "#     y_test = np.asarray(Search_y_test)\n",
    "#     misclassified = misclassified = [i for i in range(len(y_score)) if y_score[i] != y_test[i]]\n",
    "#     #rint(\"Misclassified index: \", misclassified)\n",
    "#     precision = precision_score(Search_y_test, y_score, average=None)\n",
    "#     print(\"Predicted result:\",y_score )\n",
    "#     p_class_0.append(precision[0])\n",
    "#     p_class_1.append(precision[1])\n",
    "#     recall = recall_score(Search_y_test, y_score, average=None)\n",
    "#     r_class_0.append(recall[0])\n",
    "#     r_class_1.append(recall[1])\n",
    "#     print(\"===================\",target_names[1],\"===================\")\n",
    "#     print('Average class ',target_names[0],' precision = {:.2f}'.format((sum(p_class_0)/len(p_class_0))*100))\n",
    "#     print('Average class ',target_names[1],' precision = {:.2f}'.format((sum(p_class_1)/len(p_class_1))*100))\n",
    "#     print('Average class ',target_names[0],' recall = {:.2f}'.format((sum(r_class_0)/len(r_class_0))*100))\n",
    "#     print('Average class ',target_names[1],' recall = {:.2f}'.format((sum(r_class_1)/len(r_class_1))*100))\n",
    "#     print('Accuracy: {:.2f}'.format(accuracy_score(Search_y_test, y_score)*100))\n",
    "#     #print(classification_report(Search_y_test, y_score, target_names=target_names))\n",
    "#     Search_y_test = np.nan_to_num(Search_y_test)\n",
    "#     a = pd.DataFrame(Search_y_test, columns=['Actual'])\n",
    "#     b = pd.DataFrame(y_score, columns=['Predictions'])\n",
    "#     c = pd.concat([a, b], axis=1)\n",
    "#     path_name = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/\"+target_names[1]+\"_Predictions.csv\"\n",
    "#     c.to_csv(path_name)  \n",
    "#     name = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/\"+target_names[1]+\"_misclassified.csv\"\n",
    "#     df = pd.DataFrame(misclassified) \n",
    "#     df.to_csv(name)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36b84d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Search_searchpath = '/Users/mdjavedulferdous/Desktop/TiiS/Code/result/search/model_csv/AmericanEagle.html.csv'\n",
    "Search_csv_df = pd.read_csv(Search_searchpath)\n",
    "search_URL_name = Search_csv_df[[\"URL name\"]]\n",
    "Search_X_test = Search_csv_df[[\"search_innertext\", \"search_attribute\", \"Number_of_search_word\",\"search_button_attribute_value\",\"is_button\"]]\n",
    "searchname = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/search_model.sav\"\n",
    "target_names = ['Non-Search', 'Search']\n",
    "search_URL_name = (search_URL_name[\"URL name\"][0].split('/')[-1]).split('.')[0]\n",
    "nameCSV = search_URL_name+'.csv'\n",
    "\n",
    "data1_import = evaluation(Search_X_test,searchname,target_names,search_URL_name)\n",
    "data2_import = pd.read_csv(Search_searchpath)   \n",
    "data_merge = pd.concat([data1_import, data2_import],axis=1)\n",
    "\n",
    "data_merge.to_csv(nameCSV, index = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_searchpath = '/Users/mdjavedulferdous/Desktop/TiiS/Code/result/filter_test_set_1.csv'\n",
    "filter_csv_df = pd.read_csv(filter_searchpath)\n",
    "filter_URL_name = filter_csv_df[[\"name_url\"]]\n",
    "\n",
    "filter_X_test = filter_csv_df[[\"checkBoxList\", \"NumberOfLink\",\"NumberOfInput\", \"URL_List\", \"button_List\"]]\n",
    "filter_y_test = filter_csv_df[[\"filterClass\"]]\n",
    "filtername = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/filter_model.sav\"\n",
    "target_names = ['Non-Filter', 'Filter']\n",
    "evaluation(filter_X_test,filter_y_test,filtername,target_names,filter_URL_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8325bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_searchpath = '/Users/mdjavedulferdous/Desktop/TiiS/Code/result/sort_test_set_1.csv'\n",
    "sort_csv_df = pd.read_csv(sort_searchpath)\n",
    "sort_X_test = sort_csv_df[['sort_inner','sort_attribute','option_tag_attribute_value','textCount']]\n",
    "sort_y_test = sort_csv_df[[\"sortClass\"]]\n",
    "sortname = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/sort_model.sav\"\n",
    "sort_URL_name = sort_csv_df[[\"name_url\"]]\n",
    "p_class_0, p_class_1, r_class_0, r_class_1 = [], [], [], []\n",
    "sort_model = pickle.load(open(sortname, 'rb'))\n",
    "sort_score = sort_model.predict(sort_X_test)\n",
    "target_names = ['Non-sort', 'Sort']\n",
    "\n",
    "\n",
    "y_test = np.asarray(sort_y_test)\n",
    "misclassified = misclassified = [i for i in range(len(sort_score)) if sort_score[i] != y_test[i]]\n",
    "sort_y_test = np.nan_to_num(sort_y_test)\n",
    "a = pd.DataFrame(sort_y_test, columns=['Actual'])\n",
    "b = pd.DataFrame(sort_score, columns=['Predictions'])\n",
    "c = pd.concat([a, b], axis=1)\n",
    "path_name = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/\"+target_names[1]+\"_Predictions.csv\"\n",
    "c.to_csv(path_name)  \n",
    "name = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/\"+target_names[1]+\"_misclassified.csv\"\n",
    "df = pd.DataFrame(misclassified) \n",
    "df.to_csv(name)     \n",
    "    \n",
    "precision = precision_score(sort_y_test, sort_score, average=None)\n",
    "p_class_0.append(precision[0])\n",
    "p_class_1.append(precision[1])\n",
    "recall = recall_score(sort_y_test, sort_score, average=None)\n",
    "r_class_0.append(recall[0])\n",
    "r_class_1.append(recall[1])\n",
    "print(\"===================\",target_names[1],\"===================\")\n",
    "print('Average class ',target_names[0],' precision = {:.2f}'.format((sum(p_class_0)/len(p_class_0))*100))\n",
    "print('Average class ',target_names[1],' precision = {:.2f}'.format((sum(p_class_1)/len(p_class_1))*100))\n",
    "print('Average class ',target_names[0],' recall = {:.2f}'.format((sum(r_class_0)/len(r_class_0))*100))\n",
    "print('Average class ',target_names[1],' recall = {:.2f}'.format((sum(r_class_1)/len(r_class_1))*100))\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(sort_y_test, sort_score)*100))\n",
    "target_names = ['Non-sort', 'Sort']\n",
    "#print(classification_report(sort_y_test, sort_score, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8890712",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_searchpath = '/Users/mdjavedulferdous/Desktop/TiiS/Code/result/page_test_set_1.csv'\n",
    "page_csv_df = pd.read_csv(page_searchpath)\n",
    "page_X_test = page_csv_df[['NumOfButton','NumOfLinks','commonURL','is_page','NumOfPage','NumberOfValues','navType']]\n",
    "page_y_test = page_csv_df[[\"pageClass\"]]\n",
    "pagename = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/page_model.sav\"\n",
    "page_URL_name = page_csv_df[[\"name_url\"]]\n",
    "\n",
    "p_class_0, p_class_1, r_class_0, r_class_1 = [], [], [], []\n",
    "page_model = pickle.load(open(pagename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cdeb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_score = page_model.predict(page_X_test)\n",
    "print(page_model.score(page_X_test, page_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6793fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(page_y_test, page_score, average=None)\n",
    "p_class_0.append(precision[0])\n",
    "p_class_1.append(precision[1])\n",
    "recall = recall_score(page_y_test, page_score, average=None)\n",
    "r_class_0.append(recall[0])\n",
    "r_class_1.append(recall[1])\n",
    "print(\"===================\",target_names[1],\"===================\")\n",
    "\n",
    "print('Average class ',target_names[0],' precision = {:.2f}'.format((sum(p_class_0)/len(p_class_0))*100))\n",
    "print('Average class ',target_names[1],' precision = {:.2f}'.format((sum(p_class_1)/len(p_class_1))*100))\n",
    "print('Average class ',target_names[0],' recall = {:.2f}'.format((sum(r_class_0)/len(r_class_0))*100))\n",
    "print('Average class ',target_names[1],' recall = {:.2f}'.format((sum(r_class_1)/len(r_class_1))*100))\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(page_y_test, page_score)*100))\n",
    "target_names = ['Non-Page', 'Page']\n",
    "#print(classification_report(page_y_test, page_score, target_names=target_names))\n",
    "\n",
    "\n",
    "y_test = np.asarray(page_y_test)\n",
    "misclassified = misclassified = [i for i in range(len(page_score)) if page_score[i] != y_test[i]]\n",
    "page_y_test = np.nan_to_num(page_y_test)\n",
    "a = pd.DataFrame(page_y_test, columns=['Actual'])\n",
    "b = pd.DataFrame(page_score, columns=['Predictions'])\n",
    "c = pd.concat([a, b], axis=1)\n",
    "path_name = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/\"+target_names[1]+\"_Predictions.csv\"\n",
    "c.to_csv(path_name)  \n",
    "name = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/\"+target_names[1]+\"_misclassified.csv\"\n",
    "df = pd.DataFrame(misclassified) \n",
    "df.to_csv(name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7c7f3",
   "metadata": {},
   "source": [
    "#### dataset to Model [Filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_extract_model(_name_):\n",
    "    path = '/Users/mdjavedulferdous/Desktop/TiiS/Code/result/filter/model_csv/'\n",
    "    web_name = path+_name_+'.html.csv'\n",
    "    filter_csv_df = pd.read_csv(web_name)\n",
    "    filter_URL_name = filter_csv_df[[\"name_url\"]]\n",
    "    filter_X_test = filter_csv_df[[\"checkBoxList\", \"NumberOfLink\",\"NumberOfInput\", \"URL_List\", \"button_List\"]]\n",
    "    filtername = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/filter_model_2.sav\"\n",
    "    target_names = ['Non-filter', 'filter']\n",
    "    filter_URL_name = (filter_URL_name[\"name_url\"][0].split('/')[-1]).split('.')[0]\n",
    "    nameCSV = filter_URL_name+'.csv'\n",
    "\n",
    "    data1_import = evaluation(filter_X_test,filtername,target_names,filter_URL_name)\n",
    "    data2_import = pd.read_csv(web_name)   \n",
    "    data_merge = pd.concat([data1_import, data2_import],axis=1)\n",
    "\n",
    "    data_merge.to_csv(nameCSV, index = False) \n",
    "    \n",
    "filter_GT = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/result/filter/filter_GT__1.csv\"\n",
    "filter_GT_df = pd.read_csv(filter_GT)\n",
    "name_of_csv_filter = []\n",
    "for i in range(len(filter_GT_df['name_url'])):\n",
    "    name_of_csv_filter.append((filter_GT_df['name_url'][i].split('/')[-1]).split('.')[0])\n",
    "for i in name_of_csv_filter:\n",
    "    filter_extract_model(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0ba32",
   "metadata": {},
   "source": [
    "#### dataset to Model [Page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_GT = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/result/page/page_GT__1.csv\"\n",
    "page_GT_df = pd.read_csv(page_GT)\n",
    "name_of_csv = []\n",
    "for i in range(len(page_GT_df['name_url'])):\n",
    "    name_of_csv.append((page_GT_df['name_url'][i].split('/')[-1]).split('.')[0])\n",
    "for i in name_of_csv:\n",
    "    page_extract_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76830438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_extract_model(_name_):\n",
    "    path = '/Users/mdjavedulferdous/Desktop/TiiS/Code/result/page/model_csv/'\n",
    "    web_name = path+_name_+'.html.csv'\n",
    "    page_csv_df = pd.read_csv(web_name)\n",
    "    page_URL_name = page_csv_df[[\"name_url\"]]\n",
    "    page_X_test = page_csv_df[[\"NumOfButton\", \"NumOfLinks\", \"commonURL\",\"is_page\", \"NumOfPage\", \"NumberOfValues\",\"navType\"]]\n",
    "    pagename = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/page_model_2.sav\"\n",
    "    target_names = ['Non-Search', 'Search']\n",
    "    page_URL_name = (page_URL_name[\"name_url\"][0].split('/')[-1]).split('.')[0]\n",
    "    nameCSV = page_URL_name+'.csv'\n",
    "\n",
    "    data1_import = evaluation(page_X_test,pagename,target_names,page_URL_name)\n",
    "    data2_import = pd.read_csv(web_name)   \n",
    "    data_merge = pd.concat([data1_import, data2_import],axis=1)\n",
    "\n",
    "    data_merge.to_csv(nameCSV, index = False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67efad1c",
   "metadata": {},
   "source": [
    "#### dataset to Model [Sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_extract_model(_name_):\n",
    "    path = '/Users/mdjavedulferdous/Desktop/TiiS/Code/result/sort/model_csv/'\n",
    "    web_name = path+_name_+'.html.csv'\n",
    "    sort_csv_df = pd.read_csv(web_name)\n",
    "    sort_URL_name = sort_csv_df[[\"name_url\"]]\n",
    "    sort_X_test = sort_csv_df[['sort_inner','sort_attribute','option_tag_attribute_value','textCount']]\n",
    "    sortname = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/sort_model_2.sav\"\n",
    "    target_names = ['Non-sort', 'sort']\n",
    "    sort_URL_name = (sort_URL_name[\"name_url\"][0].split('/')[-1]).split('.')[0]\n",
    "    nameCSV = sort_URL_name+'.csv'\n",
    "\n",
    "    data1_import = evaluation(sort_X_test,sortname,target_names,sort_URL_name)\n",
    "    data2_import = pd.read_csv(web_name)   \n",
    "    data_merge = pd.concat([data1_import, data2_import],axis=1)\n",
    "\n",
    "    data_merge.to_csv(nameCSV, index = False) \n",
    "    \n",
    "sort_GT = \"/Users/mdjavedulferdous/Desktop/TiiS/Code/result/sort/sort_GT__2.csv\"\n",
    "sort_GT_df = pd.read_csv(sort_GT)\n",
    "name_of_csv_sort = []\n",
    "for i in range(len(sort_GT_df['name_url'])):\n",
    "    name_of_csv_sort.append((sort_GT_df['name_url'][i].split('/')[-1]).split('.')[0])\n",
    "for i in name_of_csv_sort:\n",
    "    sort_extract_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c057591",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual    = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "predicted    = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    " \n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "cf_train_matrix = confusion_matrix(actual, predicted)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cf_train_matrix, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290be176",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('/Users/mdjavedulferdous/Downloads/temp.xlsx', index_col=None, header=None) \n",
    "actual = data[0]\n",
    "predicted = data[1]\n",
    "cm = confusion_matrix(actual,predicted)\n",
    "plot_confusion_matrix(classifier, x_test, y_test)  \n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c68c9a",
   "metadata": {},
   "source": [
    "### Filter, Page: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d877e85f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASOS', 'Bata', 'boohoo', 'eBay_Art', 'FSBO', 'Gumtree', 'JohnLewis_Partners', 'LoopNet', 'NET-A-PORTER', 'UrbanLadder']\n",
      "Total number of webpage: 100\n",
      "Number of True predicted webpage: 90\n",
      "Accuracy for filter : 90\n",
      "['eBay_Art', 'FSBO', 'Gumtree']\n",
      "Total number of webpage: 100\n",
      "Number of True predicted webpage: 97\n",
      "Accuracy for page : 97\n"
     ]
    }
   ],
   "source": [
    "name_list = []\n",
    "count = 1\n",
    "\n",
    "def accuracy_eval(_name_, feature,actual_data):\n",
    "    try:\n",
    "        name_path = \"./\"+feature+\"_after_model/\"\n",
    "        path = name_path\n",
    "        web_name = path+_name_+'.csv'\n",
    "\n",
    "        data_after_model = pd.read_csv(web_name)\n",
    "        data_after_model_name = data_after_model[\"name_url\"]\n",
    "\n",
    "        actual_data_list = actual_data.values.tolist()\n",
    "        data_after_model_list = data_after_model.values.tolist()\n",
    "        for i in range(len(actual_data_list)):\n",
    "            for j in range(len(data_after_model_list)):\n",
    "                if (actual_data_list[i][-1])==data_after_model_list[j][-1]:\n",
    "                    if actual_data_list[i][-2]!=1:\n",
    "                        print(actual_data_list[i][-2])\n",
    "                    else:\n",
    "                        if data_after_model_list[j][1]>data_after_model_list[j][0] and (actual_data_list[i][0]==data_after_model_list[j][2] and actual_data_list[i][1]==data_after_model_list[j][3] and actual_data_list[i][2]==data_after_model_list[j][4] and actual_data_list[i][3]==data_after_model_list[j][5]):\n",
    "                            if data_after_model_list[j][-1].split('/')[-1].split('.')[0] not in name_list:\n",
    "                                return data_after_model_list[j][-1].split('/')[-1].split('.')[0]\n",
    "                                name_list.append(data_after_model_list[j][-1].split('/')[-1].split('.')[0])\n",
    "    except:\n",
    "        pass  \n",
    "\n",
    "def check_feature(_feature_):\n",
    "    path = \"./result/\"+_feature_+\"/\"+_feature_+\"_GT__1.csv\"\n",
    "    actual_data = pd.read_csv(path)\n",
    "    classname = _feature_+\"Class\"\n",
    "    actual = actual_data[classname]\n",
    "    name_path_list = []\n",
    "    for i in range(len(actual_data['name_url'])):\n",
    "        name_path = (actual_data['name_url'][i].split('/')[-1]).split('.')[0]\n",
    "        name_path_list.append(name_path)\n",
    "    \n",
    "    for i in name_path_list:\n",
    "            name_list.append(accuracy_eval(i,_feature_,actual_data))\n",
    "    list_difference = []\n",
    "    for item in name_path_list:\n",
    "      if item not in name_list:\n",
    "        list_difference.append(item)\n",
    "\n",
    "    print(list_difference)\n",
    "    true_predict = (sum(x is not None for x in set(name_list)))\n",
    "    print(\"Total number of webpage:\",len(actual))\n",
    "    print(\"Number of True predicted webpage:\",true_predict)\n",
    "    print(\"Accuracy for\",_feature_,\":\",true_predict)\n",
    "check_feature(\"filter\")\n",
    "check_feature(\"page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b010340",
   "metadata": {},
   "source": [
    "### Search: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fa36ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 29]\n",
      " [ 0 71]]\n",
      "Total number of webpage: 100\n",
      "Number of True predicted webpage: 62\n",
      "Accuracy for search : 62\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHSCAYAAAA+DMuQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYQUlEQVR4nO3de7BlZXkn4N97oIkRNdJBoC8oEsDLWCVmkDFjmaiIoCY2lSoomcRqLayumYkZrLkkzJSjpRlHJk6IWOWk6sRbT4hox2iBBC9MJ8RcxNAi8UJjWhDh2CfdiWKMRgP0+eaP3mF6upve5+x9Tq/+muehVu291t57ra+r6Oq3ft+7vlWttQAADGVm6AEAAI9uihEAYFCKEQBgUIoRAGBQihEAYFCKEQBgUMeu+AWOW+feYRjA5hNfNPQQ4FHrF3ZeU4fzeg/+7d3L/m/tqhNPP2x/BskIADCoFU9GAIAVtrDnsF+yqp6W5MP7HDo9yZuS/O/R8dOS3JPkktba/Yc6l2QEAHrXFpZ/G3fJ1r7aWju7tXZ2kn+e5B+SfCzJFUm2ttbOTLJ1tH9IihEAYFrnJbmrtfaNJBuSbB4d35zkonE/Nk0DAL1bGJ9krLBXJbl29P7k1tp8krTW5qvqpHE/lowAAAeoqk1VtW2fbdMjfO+4JK9M8nuTXksyAgCda4vo8Vj6OdtsktlFfPVlSW5rre0a7e+qqjWjVGRNkt3jTiAZAYDeLSws/7Z4l+b/TdEkyfVJNo7eb0xy3bgTKEYAgIlU1WOTnJ/ko/scvjLJ+VW1Y/TZlePOY5oGAHq3AtM0i7psa/+Q5Mf3O/at7L27ZtEkIwDAoCQjANC7AVZgXU6SEQBgUJIRAOjdQD0jy0UxAgC9G34F1qmYpgEABiUZAYDOrcQKrIeTZAQAGJRkBAB613nPiGIEAHpnmgYAYHKSEQDonRVYAQAmJxkBgN513jOiGAGA3nV+N41pGgBgUJIRAOhd59M0khEAYFCSEQDoXec9I4oRAOhca9YZAQCYmGQEAHqngRUAYHKSEQDoXecNrJIRAGBQkhEA6F3nPSOKEQDo3YJbewEAJiYZAYDedT5NIxkBAAYlGQGA3nV+a69iBAB6Z5oGAGBykhEA6F3n0zSSEQBgUJIRAOhd58mIYgQAOteaFVgBACYmGQGA3nU+TSMZAQAGJRkBgN5Z9AwAYHKSEQDoXec9I4oRAOidaRoAgMlJRgCgd51P00hGAIBBSUYAoHed94woRgCgd6ZpAAAmJxkBgN5JRgAAJicZAYDedd7AKhkBgN4tLCz/tghV9cSq+khV3VlV26vqp6pqdVXdVFU7Rq8njDuPYgQAmNTVST7ZWnt6kmcn2Z7kiiRbW2tnJtk62j8k0zQA0LsBpmmq6glJfjrJa5KktfZAkgeqakOSF46+tjnJzUl+9VDnkowAAAeoqk1VtW2fbdN+Xzk9yd8keX9VfaGq3lNVxyc5ubU2nySj15PGXUsyAgC9W4Fbe1trs0lmD/GVY5P8ZJJfbq19rqquziKmZA5GMgIATGIuyVxr7XOj/Y9kb3Gyq6rWJMnodfe4EylGAKB3bWH5t3GXbO2vk9xXVU8bHTovyR1Jrk+ycXRsY5Lrxp3LNA0A9G64FVh/OcnvVtVxSe5O8trsDTq2VNVlSe5NcvG4kyhGAICJtNZuT3LOQT46bynnUYwAQO88mwYAYHKSEQDoXWtDj2AqihEA6J1pGgCAyUlGAKB3khEAgMlJRgCgdwM8tXc5KUYAoHemaQAAJicZAYDedb7OiGQEABiUZAQAeqdnBABgcpIRAOhd58mIYgQAetf5OiOmaQCAQUlGAKBzbcGtvQAAE5OMAEDvNLACAIPSwAoAMDnJCAD0TgMrAMDkJCMA0DsNrADAoDovRkzTAACDkowAQO+aBlYAgIlJRgCgd3pGAAAmJxnhoC546Qtz1VVvzTEzM3nf+6/Nr7/j3UMPCY5Kj127Oj919b/Oj570Y2kLLV+75o/y1fd+Kk985pNz7pWvzarjH5Pvzf1N/uyXfisPfe8HQw+XI1Xni54pRjjAzMxM3nX123Lhyy/N3Nx8bvnsjfn4DZ/O9u07hh4aHHUWHlrIbW/9YO7/0j059vjH5GWf/LXMf+ZLed7/fF1ue+sHs/uWO3P6q346z/w3r8gX3/GRoYfLkcqzaTjanPvc5+Suu+7J179+bx588MFs2XJdXvlzFww9LDgq/XD3d3L/l+5Jkjz0/R/m7762M49dszpP+Ik12X3LnUmSv/7Ml/PkVzx3wFHCyhqbjFTV05NsSLIuSUuyM8n1rbXtKzw2BrJ23Sm5b27nw/tz35zPuc99zoAjgkeH49efmNXPekr+9ra78p2v3pf1F/xk5j51W578s/8ij127eujhcSTrfJrmkMlIVf1qkg8lqSR/keTW0ftrq+qKlR8eQ6iqA461zu9hhyPdsY/9kbzgPZfn82+6Jg997we55d//ds56zfm58JO/llWPe0wWHnho6CHCihmXjFyW5J+11h7c92BVXZXkK0muPNiPqmpTkk1JUsf8WGZmjl+GoXK4fHNuPqeuX/vw/vp1azI/v2vAEcHRrY49Ji94z+W556N/nvs+sS1J8t2vzecPL/0fSZLHn35K1p539oAj5EjXjvJbexeSrD3I8TWjzw6qtTbbWjuntXaOQqQ/t267PWec8dScdtqpWbVqVS65ZEM+fsOnhx4WHLWe9xuvy3d37Myds594+NiP/PgT9r6pyrMu35Adv7N1oNHRhYW2/NthNC4ZeUOSrVW1I8l9o2NPTnJGktev4LgY0J49e3L5G96YG//ggzlmZiYf2Pzh3HHHXw09LDgqPencs3L6xS/I/Xfcm5fd9LYkyV++fUse/9RTctZrXpIkue8T23L3hz4z5DBhRdW4XoCqmklybvY2sFaSuSS3ttb2LOYCxx63TrMBDGDziS8aegjwqPULO685sPluBX3/v/3isv9be/wbD9+fYezdNK21hSS3HIaxAACPQhY9A4DedX5rr2IEAHp3lN9NAwCwoiQjANC7zqdpJCMAwKAkIwDQO0/tBQCYnGQEAHrXec+IYgQAOne0PygPAGBFSUYAoHemaQCAR6OquifJ3yfZk+Sh1to5VbU6yYeTnJbkniSXtNbuP9R5TNMAQO8W2vJvi/ei1trZrbVzRvtXJNnaWjszydbR/iEpRgCgd21h+bfJbUiyefR+c5KLxv1AMQIATKol+XRVfb6qNo2Ondxam0+S0etJ406iZwQAercCDayj4mLTPodmW2uz+33t+a21nVV1UpKbqurOSa6lGAEADjAqPPYvPvb/zs7R6+6q+liSc5Psqqo1rbX5qlqTZPe4a5mmAYDOtYW27Ns4VXV8VT3+n94neWmSLye5PsnG0dc2Jrlu3LkkIwDQu2HWGTk5yceqKtlbT3ywtfbJqro1yZaquizJvUkuHncixQgAsGSttbuTPPsgx7+V5LylnEsxAgC982waAIDJSUYAoHedP5tGMgIADEoyAgC96zwZUYwAQOda67sYMU0DAAxKMgIAvet8mkYyAgAMSjICAL3rPBlRjABA5xbzYLsjmWkaAGBQkhEA6J1kBABgcpIRAOhd3w/tVYwAQO80sAIATEEyAgC9k4wAAExOMgIAveu8gVUyAgAMSjICAJ3r/W4axQgA9M40DQDA5CQjANC53qdpJCMAwKAkIwDQu857RhQjANC51nkxYpoGABiUZAQAeicZAQCYnGQEADrXe8+IYgQAetd5MWKaBgAYlGQEADrX+zSNZAQAGJRkBAA613syohgBgM71XoyYpgEABiUZAYDetRp6BFORjAAAg5KMAEDn9IwAAExBMgIAnWsLffeMKEYAoHOmaQAApiAZAYDONbf2AgBMTjICAJ3rvWdEMQIAnev9bhrTNADAoCQjANC51oYewXQkIwDAxKrqmKr6QlXdMNpfXVU3VdWO0esJ486hGAGAzrWFWvZtCS5Psn2f/SuSbG2tnZlk62j/kBQjANC5oYqRqlqf5BVJ3rPP4Q1JNo/eb05y0bjzKEYAgEm9M8mvJNn35uKTW2vzSTJ6PWncSRQjANC51pZ/q6pNVbVtn23Tvtesqp9Nsru19vlpx+9uGgDgAK212SSzh/jK85O8sqpenuQxSZ5QVdck2VVVa1pr81W1JsnucdeSjABA54boGWmt/efW2vrW2mlJXpXkD1trv5jk+iQbR1/bmOS6cedSjAAAy+nKJOdX1Y4k54/2D8k0DQB0buin9rbWbk5y8+j9t5Kct5TfK0YAoHO9PyjPNA0AMCjJCAB0bmHgaZppSUYAgEFJRgCgc0M3sE5LMQIAnVvig+2OOKZpAIBBSUYAoHOtDT2C6UhGAIBBSUYAoHO994woRgCgc9YZAQCYgmQEADrX+zojkhEAYFCSEQDonFt7AQCmIBkBgM71fjeNYgQAOqeBFQBgCpIRAOicBlYAgClIRgCgcxpYgSPSJV9869BDAA4TDawAAFOQjABA53qfppGMAACDkowAQOc6v7NXMQIAvTNNAwAwBckIAHTOrb0AAFOQjABA5xaGHsCUJCMAwKAkIwDQuZa+e0YUIwDQuYXOFxoxTQMADEoyAgCdW+h8mkYyAgAMSjICAJ3TwAoADMo6IwAAU5CMAEDnep+mkYwAAIOSjABA53rvGVGMAEDnei9GTNMAAIOSjABA5zSwAgBMQTICAJ1b6DsYkYwAAMOSjABA53p/aq9iBAA614YewJRM0wAAS1ZVj6mqv6iqv6yqr1TVW0bHV1fVTVW1Y/R6wrhzKUYAoHMLK7Atwj8meXFr7dlJzk5yYVU9L8kVSba21s5MsnW0f0iKEQBgydpe3xvtrhptLcmGJJtHxzcnuWjcufSMAEDnFmqYBtaqOibJ55OckeTdrbXPVdXJrbX5JGmtzVfVSePOIxkBgM61FdiqalNVbdtn23TAdVvb01o7O8n6JOdW1bMmGb9kBAA4QGttNsnsIr/7naq6OcmFSXZV1ZpRKrImye5xv5eMAEDnhmhgraonVdUTR+9/NMlLktyZ5PokG0df25jkunHnkowAAJNYk2TzqG9kJsmW1toNVfXZJFuq6rIk9ya5eNyJFCMA0Lkhnk3TWvtikucc5Pi3kpy3lHMpRgCgc70vB69nBAAYlGQEADrn2TQAAFOQjABA54ZoYF1OkhEAYFCSEQDo3CKfsnvEUowAQOc0sAIATEEyAgCd08AKADAFyQgAdE4DKwAwqN6LEdM0AMCgJCMA0LmmgRUAYHKSEQDoXO89I4oRAOhc78WIaRoAYFCSEQDonGfTAABMQTICAJ3zbBoAgClIRgCgc73fTaMYAYDO9V6MmKYBAAYlGQGAzrm1FwBgCpIRAOhc77f2KkYAoHMaWAEApiAZAYDOaWAFAJiCZAQAOrfQeTaiGAGAzmlgBQCYgmQEADrX9ySNZAQAGJhkBAA6p2cEAGAKkhEA6Jxn0wAAg+p9nRHTNADAoCQjANC5vnMRyQgAMDDJCAB0rvdbexUjANA5DawAAFOQjABA5/rORSQjAMDAJCMA0DkNrADAoDSwAgBMQTECAJ1rK7CNU1WnVtUfVdX2qvpKVV0+Or66qm6qqh2j1xPGnUsxAgBM4qEk/6G19owkz0vyS1X1zCRXJNnaWjszydbR/iEpRgCgcwsrsI3TWptvrd02ev/3SbYnWZdkQ5LNo69tTnLRuHMpRgCgc20F/quqTVW1bZ9t0yNdv6pOS/KcJJ9LcnJrbT7ZW7AkOWnc+N1NAwAcoLU2m2R23Peq6nFJfj/JG1pr362qJV9LMQIAnRtqnZGqWpW9hcjvttY+Ojq8q6rWtNbmq2pNkt3jzmOaBgBYstobgbw3yfbW2lX7fHR9ko2j9xuTXDfuXJIRAOjcQIuePT/Jq5N8qapuHx37L0muTLKlqi5Lcm+Si8edSDECACxZa+1PkzxSg8h5SzmXYgQAOtf3YvCKEQDonmfTAABMQTLCQV3w0hfmqqvemmNmZvK+91+bX3/Hu4ceEhyVvv6NufzHN7394f25nfN5/etenZOedGL+13uvyd3fuC/X/vY786xnnDXgKDnSDXVr73JRjHCAmZmZvOvqt+XCl1+aubn53PLZG/PxGz6d7dt3DD00OOo89Snr8/ub9xb7e/bsyYsvenXO+5l/mR/88B/zzv/+X/OWd7xr4BHCylOMcIBzn/uc3HXXPfn61+9NkmzZcl1e+XMXKEZghd2y7facum5N1p5y8tBDoTNNzwhHm7XrTsl9czsf3p/75nzWrj1lwBHBo8Mntv5xXv6Snxl6GHRoiAflLSfFCAc42HMFWuu76oYj3YMPPpib//RzeemLXzD0UOCwm7gYqarXHuKzh5/0t7Dw/UkvwUC+OTefU9evfXh//bo1mZ/fNeCI4Oj3J7dsyzPO+omcuPqEoYdCh1biqb2H0zTJyFse6YPW2mxr7ZzW2jkzM8dPcQmGcOu223PGGU/NaaedmlWrVuWSSzbk4zd8euhhwVHtxptuzsvPf+HQw4BBHLKBtaq++EgfJdFhdZTas2dPLn/DG3PjH3wwx8zM5AObP5w77viroYcFR60f/PCH+eytX8ibf+XfPXzs//zxn+Xtv/lb+fZ3/i7/9j+9OU8/8/TM/ubbBhwlR7Leb+2tQ/UCVNWuJBckuX//j5L8eWtt7YG/+v8de9w6zQYwgB/s/JOhhwCPWqtOPP2RntmyIl79lJ9f9n9rf+cbHz1sf4Zxt/bekORxrbXb9/+gqm5eiQEBAI8uhyxGWmuXHeKzf7X8wwEAlqr3KQi39gIAg7ICKwB0zlN7AQCmIBkBgM71/mwaxQgAdK73dUZM0wAAg5KMAEDnNLACAExBMgIAndPACgAMSgMrAMAUJCMA0LnW+p6mkYwAAIOSjABA53q/tVcxAgCd08AKADAFyQgAdK73dUYkIwDAoCQjANC53htYJSMAwKAkIwDQud4XPVOMAEDn3NoLADAFyQgAdM6tvQAAU5CMAEDner+1VzECAJ3r/W4a0zQAwKAkIwDQud6naSQjAMCgJCMA0Lneb+1VjABA5xY0sAIATE4yAgCd6zsXkYwAAAOTjABA59zaCwAwBckIAHROMgIADKq1tuzbOFX1vqraXVVf3ufY6qq6qap2jF5PWMz4FSMAwCQ+kOTC/Y5dkWRra+3MJFtH+2MpRgCgcwtpy76N01r7TJJv73d4Q5LNo/ebk1y0mPErRgCA5XJya20+SUavJy3mRxpYAaBzK/FsmqralGTTPodmW2uzy36hKEYAoHuLaTid4JyzSZZafOyqqjWttfmqWpNk92J+ZJoGAFgu1yfZOHq/Mcl1i/mRZAQAOjfEOiNVdW2SFyY5sarmkrw5yZVJtlTVZUnuTXLxYs6lGAEAlqy1dukjfHTeUs+lGAGAzq1Ez8jhpBgBgM5ZDh4AYAqSEQDo3EqsM3I4SUYAgEFJRgCgcwudN7BKRgCAQUlGAKBzvfeMKEYAoHOmaQAApiAZAYDO9T5NIxkBAAYlGQGAzvXeM6IYAYDOmaYBAJiCZAQAOtf7NI1kBAAYlGQEADrXe8+IYgQAOtfawtBDmIppGgBgUJIRAOjcQufTNJIRAGBQkhEA6Fxzay8AwOQkIwDQud57RhQjANA50zQAAFOQjABA5zybBgBgCpIRAOicZ9MAAIPSwAoAMAXJCAB0rvd1RiQjAMCgJCMA0Lnee0YUIwDQOeuMAABMQTICAJ3rfZpGMgIADEoyAgCdc2svAMAUJCMA0Lnee0YUIwDQObf2AgBMQTICAJ1rGlgBACYnGQGAzvXeM6IYAYDO9X43jWkaAGBQkhEA6JwGVgCAKUhGAKBzekYAgEG11pZ9W4yqurCqvlpVX6uqKyYdv2IEAFiyqjomybuTvCzJM5NcWlXPnORcihEA6FxbgW0Rzk3ytdba3a21B5J8KMmGScavGAEAJrEuyX377M+Nji3ZijewPvTAN2ulr8HKqapNrbXZoccBjzb+7rEUK/FvbVVtSrJpn0Oz+/0/ebBrTtRJKxlhnE3jvwKsAH/3GFRrbba1ds4+2/7F8VySU/fZX59k5yTXUowAAJO4NcmZVfXUqjouyauSXD/JiawzAgAsWWvtoap6fZJPJTkmyftaa1+Z5FyKEcYxZw3D8HePI15r7cYkN057nup91TYAoG96RgCAQSlGOKjlWuIXWJqqel9V7a6qLw89FjhcFCMcYDmX+AWW7ANJLhx6EHA4KUY4mGVb4hdYmtbaZ5J8e+hxwOGkGOFglm2JXwAYRzHCwSzbEr8AMI5ihINZtiV+AWAcxQgHs2xL/ALAOIoRDtBaeyjJPy3xuz3JlkmX+AWWpqquTfLZJE+rqrmqumzoMcFKswIrADAoyQgAMCjFCAAwKMUIADAoxQgAMCjFCAAwKMUIADAoxQgAMCjFCAAwqP8LAc1yD1lR0mIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_list, metrics = [], []\n",
    "count = 1\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "def accuracy_eval(_name_, feature,actual_data):\n",
    "    try:\n",
    "        \n",
    "        name_path = \"./\"+feature+\"_after_model/\"\n",
    "        path = name_path\n",
    "        web_name = path+_name_+'.csv'\n",
    "\n",
    "        data_after_model = pd.read_csv(web_name)\n",
    "        data_after_model_name = data_after_model[\"URL name\"]\n",
    "\n",
    "        actual_data_list = actual_data.values.tolist()\n",
    "        data_after_model_list = data_after_model.values.tolist()\n",
    "        for i in range(len(actual_data_list)):\n",
    "            for j in range(len(data_after_model_list)):\n",
    "                if (actual_data_list[i][-1])==data_after_model_list[j][-1]:\n",
    "                     if (actual_data_list[i][-2]==1 and data_after_model_list[j][1]>data_after_model_list[j][0])  :\n",
    "                            if data_after_model_list[j][-1].split('/')[-1].split('.')[0] not in name_list:\n",
    "                                name_list.append(data_after_model_list[j][-1].split('/')[-1].split('.')[0])\n",
    "                                #print((name_list))\n",
    "                                return data_after_model_list[j][-1].split('/')[-1].split('.')[0]\n",
    "    except:\n",
    "      pass  \n",
    "\n",
    "def check_feature(_feature_):\n",
    "    path = \"./result/\"+_feature_+\"/\"+_feature_+\"_GT__1.csv\"\n",
    "    actual_data = pd.read_csv(path)\n",
    "#     classname = _feature_+\"Class\"\n",
    "    actual = actual_data[\"sClass\"]\n",
    "    name_path_list = []\n",
    "    for i in range(len(actual_data['URL name'])):\n",
    "        name_path = (actual_data['URL name'][i].split('/')[-1]).split('.')[0]\n",
    "        name_path_list.append(name_path)\n",
    "    \n",
    "    for i in name_path_list:\n",
    "            name_list.append(accuracy_eval(i,_feature_,actual_data))\n",
    "            list_difference = []\n",
    "            for item in name_path_list:\n",
    "                if item not in name_list:\n",
    "                    list_difference.append(item)\n",
    "                #print(i,item)\n",
    "            if None in name_path_list:\n",
    "                metrics.append(0)\n",
    "            else:\n",
    "                metrics.append(1)\n",
    "#     print(list_difference)\n",
    "    actual_data_list = actual.values.tolist()\n",
    "    \n",
    "    cf_train_matrix = confusion_matrix(actual_data_list, metrics)\n",
    "    print(cf_train_matrix)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cf_train_matrix, annot=True, fmt='d')\n",
    "    true_predict = (sum(x is not None for x in set(name_list)))\n",
    "    print(\"Total number of webpage:\",len(actual))\n",
    "    print(\"Number of True predicted webpage:\",true_predict)\n",
    "    print(\"Accuracy for\",_feature_,\":\",true_predict)\n",
    "check_feature(\"search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce749937",
   "metadata": {},
   "source": [
    "### Sort: Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23dd3299",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  9]\n",
      " [ 0 91]]\n",
      "Total number of webpage: 100\n",
      "Number of True predicted webpage: 26\n",
      "Accuracy for sort : 26\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_list, metrics = [], []\n",
    "count = 1\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "def accuracy_eval(_name_, feature,actual_data):\n",
    "        name_path = \"./\"+feature+\"_after_model/\"\n",
    "        path = name_path\n",
    "        web_name = path+_name_+'.csv'\n",
    "        \n",
    "        data_after_model = pd.read_csv(web_name)\n",
    "        data_after_model_name = data_after_model[\"name_url\"]\n",
    "        actual_data_list = actual_data.values.tolist()\n",
    "        data_after_model_list = data_after_model.values.tolist()\n",
    "\n",
    "        for i in range(len(actual_data_list)):\n",
    "            for j in range(len(data_after_model_list)):\n",
    "                if (actual_data_list[i][-1])==data_after_model_list[j][-1] and actual_data_list[i][-3]==1:\n",
    "                    if data_after_model_list[j][1]>data_after_model_list[j][0]:\n",
    "                        if (actual_data_list[i][0]==data_after_model_list[j][2] and actual_data_list[i][1]==data_after_model_list[j][3] ):\n",
    "                            if data_after_model_list[j][-1].split('/')[-1].split('.')[0] not in name_list:\n",
    "                                return data_after_model_list[j][-1].split('/')[-1].split('.')[0]\n",
    "                                name_list.append(data_after_model_list[j][-1].split('/')[-1].split('.')[0])\n",
    "                                print(name_list)\n",
    "\n",
    "\n",
    "def check_feature(_feature_):\n",
    "    path = \"./result/\"+_feature_+\"/\"+_feature_+\"_GT__2.csv\"\n",
    "    actual_data = pd.read_csv(path)\n",
    "    classname = _feature_+\"Class\"\n",
    "    actual = actual_data[classname]\n",
    "    name_path_list = []\n",
    "    for i in range(len(actual_data['name_url'])):\n",
    "        name_path = (actual_data['name_url'][i].split('/')[-1]).split('.')[0]\n",
    "        name_path_list.append(name_path)\n",
    "    \n",
    "    for i in name_path_list:\n",
    "            name_list.append(accuracy_eval(i,_feature_,actual_data))\n",
    "            list_difference = []\n",
    "            for item in name_path_list:\n",
    "                if item not in name_list:\n",
    "                    list_difference.append(item)\n",
    "                #print(i,item)\n",
    "            if None in name_path_list:\n",
    "                metrics.append(0)\n",
    "            else:\n",
    "                metrics.append(1)\n",
    "#     print(list_difference)\n",
    "    actual_data_list = actual.values.tolist()\n",
    "    \n",
    "    cf_train_matrix = confusion_matrix(actual_data_list, metrics)\n",
    "    print(cf_train_matrix)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cf_train_matrix, annot=True, fmt='d')\n",
    "    true_predict = (sum(x is not None for x in set(name_list)))\n",
    "    print(\"Total number of webpage:\",len((name_list)))\n",
    "    print(\"Number of True predicted webpage:\",true_predict)\n",
    "    print(\"Accuracy for\",_feature_,\":\",true_predict)\n",
    "check_feature(\"sort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916bbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
