{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662d4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init import *\n",
    "from allURL import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b36abda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mdjavedulferdous/Documents/Dataset/New Dataset\n"
     ]
    }
   ],
   "source": [
    "cd \"/Users/mdjavedulferdous/Documents/Dataset/New Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c733b1",
   "metadata": {},
   "source": [
    "### Extract Search node\n",
    "<hr /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2561197",
   "metadata": {},
   "source": [
    "#### Saved Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "587d3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract(_URL_):\n",
    "    myFile=open(_URL_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    name  = []\n",
    "    formList= 0\n",
    "    name.append(urllist[i].split('/')[-2])\n",
    "    try:\n",
    "            for tests in soup.findAll('form'):\n",
    "                my_attributes = tests.attrs\n",
    "                if(tests.has_attr('action')) ==True:\n",
    "                #print(my_attributes)\n",
    "                    formList=1\n",
    "                else:\n",
    "                    formList=0\n",
    "                \n",
    "                if my_attributes.has_attr('action')==True:\n",
    "                    pass #print(my_attributes)  \n",
    "    except:\n",
    "            pass\n",
    "    return formList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dd3ff07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count, searchCount= 1,0\n",
    "newList,formList = [],[]\n",
    "for i in range(0,209):\n",
    "    formList.append(Extract(urllist[i]))\n",
    "    count +=1 \n",
    "for j in range(0,209):    \n",
    "    if formList[j]==1:\n",
    "        searchCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7855c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort\n",
      "================================\n",
      "Accuracy: 100.0%\n",
      "Number of True result: 209\n",
      "Number of Total result: 208\n"
     ]
    }
   ],
   "source": [
    "def Read_CSV(completeName, className, counterVar):\n",
    "    df = pd.read_csv(completeName)\n",
    "    falseSearch = (df[className]==1).value_counts()[0]\n",
    "    trueSearch = (df[className]==1).value_counts()[1]\n",
    "    total = len(df[className])\n",
    "    print(className.split('Class')[0])\n",
    "    print(\"================================\")\n",
    "    if counterVar<trueSearch:\n",
    "        print('Accuracy: {:.1f}%'.format(((counterVar)/trueSearch)*100))\n",
    "        \n",
    "    else:\n",
    "        print('Accuracy: {:.1f}%'.format(((counterVar)/counterVar)*100))\n",
    "    #print('Number of False result: {:.1f}'.format(falseSearch))\n",
    "    print('Number of True result: {:.0f}'.format(counterVar))\n",
    "    print('Number of Total result: {:.0f}'.format(trueSearch))\n",
    "def main():\n",
    "    save_path = '/Users/mdjavedulferdous/Desktop/TiiS/Code/'\n",
    "    search_name = \"search.csv\"\n",
    "    page_name = 'page.csv'\n",
    "    sort_name = 'sort.csv'\n",
    "    filter_name = 'filter.csv'\n",
    "    searchName = os.path.join(save_path, search_name)\n",
    "    pageName = os.path.join(save_path, page_name)\n",
    "    sortName = os.path.join(save_path, sort_name)\n",
    "    filterName = os.path.join(save_path, filter_name)\n",
    "    #Read_CSV(searchName,'sClass',searchCount)\n",
    "    #Read_CSV(pageName,'pageClass')\n",
    "    Read_CSV(sortName,'sortClass',count)\n",
    "    #Read_CSV(filterName,'filterClass')\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec61841",
   "metadata": {},
   "source": [
    "#### Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaaeecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import clear\n",
    "from selenium import webdriver\n",
    "\n",
    "DRIVER_PATH = '/Users/mdjavedulferdous/Desktop/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "URL1 = 'https://www.amazon.com/s?k=apple+watch&i=electronics&crid=3MR9PVJ318602&sprefix=%2Celectronics%2C328&ref=nb_sb_ss_recent_1_0_recent'\n",
    "URL2 = 'https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2380057.m570.l1313&_nkw=laptop&_sacat=175672'\n",
    "def ExtractLive(_URL_):\n",
    "    driver.get(_URL_)\n",
    "    #print(driver.page_source)\n",
    "    soup_level1=BeautifulSoup(driver.page_source, 'lxml')\n",
    "    name, formList = [], []\n",
    "\n",
    "    try:\n",
    "                for tests in soup_level1.findAll('form'):\n",
    "                    my_attributes = tests.attrs\n",
    "                    print(my_attributes)                \n",
    "                    if my_attributes.has_attr('action')==True:\n",
    "                        pass #print(my_attributes)  \n",
    "\n",
    "    except:\n",
    "                pass\n",
    "ExtractLive(URL1)\n",
    "ExtractLive(URL2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294fdbe",
   "metadata": {},
   "source": [
    "### Extract Sort node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a238e7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adenandanais\n",
      "adidas\n",
      "aeropostale\n",
      "agoda\n",
      "airbnb\n",
      "aldoshoes\n",
      "Amazon\n",
      "amextravel\n",
      "artpal\n",
      "asos\n",
      "autoanything\n",
      "Avon\n",
      "bagborroworsteal\n",
      "barnesandnoble\n",
      "benetton\n",
      "bestbuy\n",
      "besttravelcoupon\n",
      "bettycrocker\n",
      "bhphotovideo\n",
      "bigbasket\n",
      "bigstockphoto\n",
      "bodenusa\n",
      "bonanza\n",
      "booking\n",
      "booksamillion\n",
      "bulq\n",
      "buybuybaby\n",
      "canstockphoto\n",
      "carid\n",
      "carparts\n",
      "cartier\n",
      "cdw\n",
      "cheapoair\n",
      "cheaptickets\n",
      "chewy\n",
      "childrensplace\n",
      "choicehotels\n",
      "coca-cola\n",
      "concordsupplies\n",
      "cookieskids\n",
      "costco\n",
      "costcotravel\n",
      "countrystore\n",
      "craigslist\n",
      "dayuse\n",
      "decluttr\n",
      "departmentstoreliquidations\n",
      "depositphotos\n",
      "dickssportinggoods\n",
      "discovery\n",
      "doverpublications\n",
      "dreamstime\n",
      "drupal\n",
      "dunhamssports\n",
      "eastbay\n",
      "ebay\n",
      "ebid\n",
      "ebookers\n",
      "ecrater\n",
      "entirelypets\n",
      "etsy\n",
      "expedia\n",
      "fandango\n",
      "fashionphile\n",
      "fnp\n",
      "fragrancenet\n",
      "freedigitalphotos\n",
      "freshdirect\n",
      "frys\n",
      "gamestop\n",
      "gazelle\n",
      "gbyliquidations\n",
      "getaroom\n",
      "gettyimages\n",
      "giantfood\n",
      "glyde\n",
      "google shopping\n",
      "groupon\n",
      "gymboree\n",
      "hallmark\n",
      "hersheys\n",
      "hilton\n",
      "hipcamp\n",
      "homedepot\n",
      "hometogo\n",
      "hopper\n",
      "hoseasons\n",
      "hostelworld\n",
      "hotelscombined\n",
      "hoteltonight\n",
      "hotwire\n",
      "hrs\n",
      "hurb\n",
      "hyatt\n",
      "icing\n",
      "ihg\n",
      "ikea\n",
      "indiger\n",
      "jalan\n",
      "jcpenney\n",
      "jossandmain\n",
      "kayak\n",
      "kohls\n",
      "kroger\n",
      "kushiesonline\n",
      "laterooms\n",
      "lego\n",
      "leprix\n",
      "liquidation\n",
      "listia\n",
      "lonelyplanet\n",
      "lookfantastic\n",
      "maccosmetics\n",
      "macys\n",
      "marriott\n",
      "marykay\n",
      "mercari\n",
      "metmuseum\n",
      "microcenter\n",
      "momondo\n",
      "mrandmrssmith\n",
      "mynavyexchange\n",
      "nba\n",
      "neutrogena\n",
      "newbalance\n",
      "nflshop\n",
      "nhc\n",
      "nike\n",
      "officedepot\n",
      "officesupplynow\n",
      "oncewed\n",
      "onetravel\n",
      "onlinesports\n",
      "oodle\n",
      "orbitz\n",
      "origins\n",
      "overstock\n",
      "oyorooms\n",
      "petco\n",
      "petsmart\n",
      "petsupplies\n",
      "petsuppliesplus\n",
      "poppin\n",
      "poshmark\n",
      "powells\n",
      "premierinn\n",
      "preownedweddingdresses\n",
      "priceline\n",
      "pruvo\n",
      "puma\n",
      "redbubble\n",
      "reebok\n",
      "riteaid\n",
      "roomertravel\n",
      "rubylane\n",
      "saatchiart\n",
      "safeway\n",
      "sears\n",
      "sebamedusa\n",
      "secondsale\n",
      "sellcell\n",
      "sephora\n",
      "shopbeergear\n",
      "shopbop\n",
      "shopdisney\n",
      "shopgoodwill\n",
      "shopjustice\n",
      "shopthesalvationarmy\n",
      "shopzilla\n",
      "shutterstock\n",
      "sisley\n",
      "skyscanner\n",
      "spalding\n",
      "sportsdirect\n",
      "sportsunlimitedinc\n",
      "staples\n",
      "swansonvitamins\n",
      "sykescottages\n",
      "szallas\n",
      "tabasco\n",
      "tablethotels\n",
      "target\n",
      "tenthousandvillages\n",
      "thellegance\n",
      "thenewyorkdogshop\n",
      "thomascook\n",
      "thredup\n",
      "thriftbooks\n",
      "tias\n",
      "tigerdirect\n",
      "t-mobile\n",
      "toofaced\n",
      "totalwine\n",
      "tradesy\n",
      "travelocity\n",
      "travelzoo\n",
      "tripadvisor\n",
      "trivago\n",
      "usps\n",
      "veneretravel\n",
      "verizon\n",
      "vitaminshoppe\n",
      "vitaminworld\n",
      "vrbo\n",
      "walgreens\n",
      "walmart\n",
      "wayfair\n",
      "wotif\n",
      "wyndhamhotels\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "def Extract(_URL_):\n",
    "    listOfTag = [ \"div\",'select', 'ul']\n",
    "    myFile=open(_URL_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    name = []\n",
    "    sort= 0 \n",
    "    \n",
    "    try:\n",
    "            for tests in soup.findAll('select'):\n",
    "                if bool(re.search('sort', str(tests))) ==True or bool(re.search('Sort By', str(tests))) ==True or bool(re.search('Best Match', str(tests))) ==True:\n",
    "                    if(tests.find('option')):\n",
    "                        sort=1\n",
    "                    else:\n",
    "                        sort=0\n",
    "                \n",
    "    except:\n",
    "            pass\n",
    "    return formList\n",
    "\n",
    "count= 0\n",
    "for i in range(0,209):\n",
    "    print(urllist[i].split('/')[-2])\n",
    "    Extract(urllist[i])\n",
    "    count +=1 \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5097271",
   "metadata": {},
   "source": [
    "### Extract Page node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34649b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def search(myDict, search1):\n",
    "    search.a=[]\n",
    "    for key, value in myDict.items():\n",
    "        if search1 in value:\n",
    "            search.a.append(key)\n",
    "    return len(search.a)\n",
    "\n",
    "def Extract(_URL_):\n",
    "    listOfTag = [ \"div\", \"nav\",\"li\",\"ul\",\"span\", \"section\", \"button\", \"tr\", \"footer\", \"a\", \"pagination\", \"b\" ]\n",
    "    myFile=open(_URL_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    name, pageList = [], []\n",
    "    name.append(urllist[i].split('/')[-2])\n",
    "    try:\n",
    "        for j in listOfTag:\n",
    "            for tests in soup.findAll(j):\n",
    "                    my_attributes = tests.attrs\n",
    "                    titles = soup.find_all('div',attrs = {'span'})\n",
    "                    print(titles)\n",
    "                    if my_attributes != \" \":\n",
    "                        if tests.find('page')!= -1:\n",
    "                            #print(name,j,list(tests))\n",
    "                            pageList.append(my_attributes)\n",
    "\n",
    "    except:\n",
    "            pass\n",
    "    return name, pageList\n",
    "\n",
    "count= 1\n",
    "for i in range(6,7):\n",
    "    name, pageList = Extract(urllist[i])\n",
    "    count +=1 \n",
    "    #print(name, pageList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c153f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
