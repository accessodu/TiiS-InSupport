{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662d4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init import *\n",
    "from allURL import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b36abda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mdjavedulferdous/Documents/Dataset/New Dataset\n"
     ]
    }
   ],
   "source": [
    "cd \"/Users/mdjavedulferdous/Documents/Dataset/New Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c733b1",
   "metadata": {},
   "source": [
    "### Extract Search node\n",
    "<hr /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2561197",
   "metadata": {},
   "source": [
    "#### Saved Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "587d3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract(_URL_):\n",
    "    myFile=open(_URL_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    name  = []\n",
    "    formList= 0\n",
    "    name.append(urllist[i].split('/')[-2])\n",
    "    try:\n",
    "            for tests in soup.findAll('form'):\n",
    "                my_attributes = tests.attrs\n",
    "                if(tests.has_attr('action')) ==True:\n",
    "                #print(my_attributes)\n",
    "                    formList=1\n",
    "                else:\n",
    "                    formList=0\n",
    "                \n",
    "                if my_attributes.has_attr('action')==True:\n",
    "                    pass #print(my_attributes)  \n",
    "    except:\n",
    "            pass\n",
    "    return formList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dd3ff07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1]\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "count, searchCount= 1,0\n",
    "newList,formList = [],[]\n",
    "for i in range(0,209):\n",
    "    formList.append(Extract(urllist[i]))\n",
    "    count +=1 \n",
    "print(formList)    \n",
    "for j in range(0,209):    \n",
    "    if formList[j]==1:\n",
    "        searchCount+=1\n",
    "print(searchCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_CSV(completeName, className):\n",
    "    df = pd.read_csv(completeName)\n",
    "    falseSearch = (df[className]==1).value_counts()[0]\n",
    "    trueSearch = (df[className]==1).value_counts()[1]\n",
    "    total = len(df[className])\n",
    "    print(className.split('Class')[0])\n",
    "    print(\"================================\")\n",
    "    print('Accuracy: {:.1f}%'.format(((searchCount)/trueSearch)*100))\n",
    "    #print('Number of False result: {:.1f}'.format(falseSearch))\n",
    "    print('Number of True result: {:.0f}'.format(searchCount))\n",
    "    print('Number of Total result: {:.0f}'.format(trueSearch))\n",
    "def main():\n",
    "    save_path = '/Users/mdjavedulferdous/Desktop/TiiS/Code/'\n",
    "    search_name = \"search.csv\"\n",
    "    page_name = 'page.csv'\n",
    "    sort_name = 'sort.csv'\n",
    "    filter_name = 'filter.csv'\n",
    "    searchName = os.path.join(save_path, search_name)\n",
    "    pageName = os.path.join(save_path, page_name)\n",
    "    sortName = os.path.join(save_path, sort_name)\n",
    "    filterName = os.path.join(save_path, filter_name)\n",
    "    Read_CSV(searchName,'sClass')\n",
    "    #Read_CSV(pageName,'pageClass')\n",
    "    #Read_CSV(sortName,'sortClass')\n",
    "    #Read_CSV(filterName,'filterClass')\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec61841",
   "metadata": {},
   "source": [
    "#### Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaaeecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import clear\n",
    "from selenium import webdriver\n",
    "\n",
    "DRIVER_PATH = '/Users/mdjavedulferdous/Desktop/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "URL1 = 'https://www.amazon.com/s?k=apple+watch&i=electronics&crid=3MR9PVJ318602&sprefix=%2Celectronics%2C328&ref=nb_sb_ss_recent_1_0_recent'\n",
    "URL2 = 'https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2380057.m570.l1313&_nkw=laptop&_sacat=175672'\n",
    "def ExtractLive(_URL_):\n",
    "    driver.get(_URL_)\n",
    "    #print(driver.page_source)\n",
    "    soup_level1=BeautifulSoup(driver.page_source, 'lxml')\n",
    "    name, formList = [], []\n",
    "\n",
    "    try:\n",
    "                for tests in soup_level1.findAll('form'):\n",
    "                    my_attributes = tests.attrs\n",
    "                    print(my_attributes)                \n",
    "                    if my_attributes.has_attr('action')==True:\n",
    "                        pass #print(my_attributes)  \n",
    "\n",
    "    except:\n",
    "                pass\n",
    "ExtractLive(URL1)\n",
    "ExtractLive(URL2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294fdbe",
   "metadata": {},
   "source": [
    "### Extract Sort node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract(_URL_):\n",
    "    listOfTag = [ \"div\",'select', 'ul']\n",
    "    myFile=open(_URL_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    name, formList = [], []\n",
    "    name.append(urllist[i].split('/')[-2])\n",
    "    try:\n",
    "            for tests in soup.findAll('select'):\n",
    "                if bool(re.search('sort', str(tests))) ==True:\n",
    "                    print(name,tests)\n",
    "                \n",
    "    except:\n",
    "            pass\n",
    "    return name, formList\n",
    "\n",
    "count= 1\n",
    "for i in range(0,20):\n",
    "    name, formList = Extract(urllist[i])\n",
    "    count +=1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5097271",
   "metadata": {},
   "source": [
    "### Extract Page node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34649b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def search(myDict, search1):\n",
    "    search.a=[]\n",
    "    for key, value in myDict.items():\n",
    "        if search1 in value:\n",
    "            search.a.append(key)\n",
    "    return len(search.a)\n",
    "\n",
    "def Extract(_URL_):\n",
    "    listOfTag = [ \"div\", \"nav\",\"li\",\"ul\",\"span\", \"section\", \"button\", \"tr\", \"footer\", \"a\", \"pagination\", \"b\" ]\n",
    "    myFile=open(_URL_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    name, pageList = [], []\n",
    "    name.append(urllist[i].split('/')[-2])\n",
    "    try:\n",
    "        for j in listOfTag:\n",
    "            for tests in soup.findAll(j):\n",
    "                    my_attributes = tests.attrs\n",
    "                    titles = soup.find_all('div',attrs = {'span'})\n",
    "                    print(titles)\n",
    "                    if my_attributes != \" \":\n",
    "                        if tests.find('page')!= -1:\n",
    "                            #print(name,j,list(tests))\n",
    "                            pageList.append(my_attributes)\n",
    "\n",
    "    except:\n",
    "            pass\n",
    "    return name, pageList\n",
    "\n",
    "count= 1\n",
    "for i in range(6,7):\n",
    "    name, pageList = Extract(urllist[i])\n",
    "    count +=1 \n",
    "    #print(name, pageList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c153f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
