{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662d4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init import *\n",
    "from allURL import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b36abda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mdjavedulferdous/Documents/Dataset/New Dataset\n"
     ]
    }
   ],
   "source": [
    "cd \"/Users/mdjavedulferdous/Documents/Dataset/New Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c733b1",
   "metadata": {},
   "source": [
    "### Extract Search node\n",
    "<hr /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2561197",
   "metadata": {},
   "source": [
    "#### Saved Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "587d3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract(_URL_):\n",
    "    myFile=open(_URL_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    name, formList = [], []\n",
    "    name.append(urllist[i].split('/')[-2])\n",
    "    try:\n",
    "            for tests in soup.findAll('form'):\n",
    "                my_attributes = tests.attrs\n",
    "                #print(my_attributes)\n",
    "                formList.append(my_attributes)\n",
    "                \n",
    "                if my_attributes.has_attr('action')==True:\n",
    "                    pass #print(my_attributes)  \n",
    "    except:\n",
    "            pass\n",
    "    return name, formList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7dd3ff07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['adenandanais'], 1],\n",
       " [['adidas'], 1],\n",
       " [['aeropostale'], 0],\n",
       " [['agoda'], 1],\n",
       " [['airbnb'], 1],\n",
       " [['aldoshoes'], 0],\n",
       " [['Amazon'], 1],\n",
       " [['amextravel'], 0],\n",
       " [['artpal'], 1],\n",
       " [['asos'], 1],\n",
       " [['autoanything'], 1],\n",
       " [['Avon'], 1],\n",
       " [['bagborroworsteal'], 1],\n",
       " [['barnesandnoble'], 1],\n",
       " [['benetton'], 1],\n",
       " [['bestbuy'], 1],\n",
       " [['besttravelcoupon'], 0],\n",
       " [['bettycrocker'], 1],\n",
       " [['bhphotovideo'], 1],\n",
       " [['bigbasket'], 1],\n",
       " [['bigstockphoto'], 1],\n",
       " [['bodenusa'], 1],\n",
       " [['bonanza'], 0],\n",
       " [['booking'], 1],\n",
       " [['booksamillion'], 0],\n",
       " [['bulq'], 1],\n",
       " [['buybuybaby'], 0],\n",
       " [['canstockphoto'], 1],\n",
       " [['carid'], 1],\n",
       " [['carparts'], 0],\n",
       " [['cartier'], 0],\n",
       " [['cdw'], 1],\n",
       " [['cheapoair'], 0],\n",
       " [['cheaptickets'], 1],\n",
       " [['chewy'], 1],\n",
       " [['childrensplace'], 1],\n",
       " [['choicehotels'], 1],\n",
       " [['coca-cola'], 0],\n",
       " [['concordsupplies'], 1],\n",
       " [['cookieskids'], 1],\n",
       " [['costco'], 0],\n",
       " [['costcotravel'], 0],\n",
       " [['countrystore'], 1],\n",
       " [['craigslist'], 0],\n",
       " [['dayuse'], 1],\n",
       " [['decluttr'], 1],\n",
       " [['departmentstoreliquidations'], 0],\n",
       " [['depositphotos'], 1],\n",
       " [['dickssportinggoods'], 1],\n",
       " [['discovery'], 1],\n",
       " [['doverpublications'], 0],\n",
       " [['dreamstime'], 0],\n",
       " [['drupal'], 0],\n",
       " [['dunhamssports'], 0],\n",
       " [['eastbay'], 1],\n",
       " [['ebay'], 1],\n",
       " [['ebid'], 0],\n",
       " [['ebookers'], 1],\n",
       " [['ecrater'], 1],\n",
       " [['entirelypets'], 0],\n",
       " [['etsy'], 0],\n",
       " [['expedia'], 1],\n",
       " [['fandango'], 1],\n",
       " [['fashionphile'], 0],\n",
       " [['fnp'], 1],\n",
       " [['fragrancenet'], 1],\n",
       " [['freedigitalphotos'], 1],\n",
       " [['freshdirect'], 1],\n",
       " [['frys'], 1],\n",
       " [['gamestop'], 1],\n",
       " [['gazelle'], 1],\n",
       " [['gbyliquidations'], 1],\n",
       " [['getaroom'], 0],\n",
       " [['gettyimages'], 0],\n",
       " [['giantfood'], 1],\n",
       " [['glyde'], 0],\n",
       " [['google shopping'], 1],\n",
       " [['groupon'], 1],\n",
       " [['gymboree'], 1],\n",
       " [['hallmark'], 1],\n",
       " [['hersheys'], 1],\n",
       " [['hilton'], 1],\n",
       " [['hipcamp'], 1],\n",
       " [['homedepot'], 1],\n",
       " [['hometogo'], 1],\n",
       " [['hopper'], 1],\n",
       " [['hoseasons'], 1],\n",
       " [['hostelworld'], 0],\n",
       " [['hotelscombined'], 1],\n",
       " [['hoteltonight'], 1],\n",
       " [['hotwire'], 0],\n",
       " [['hrs'], 0],\n",
       " [['hurb'], 0],\n",
       " [['hyatt'], 1],\n",
       " [['icing'], 1],\n",
       " [['ihg'], 0],\n",
       " [['ikea'], 1],\n",
       " [['indiger'], 1],\n",
       " [['jalan'], 0],\n",
       " [['jcpenney'], 1],\n",
       " [['jossandmain'], 1],\n",
       " [['kayak'], 1],\n",
       " [['kohls'], 1],\n",
       " [['kroger'], 1],\n",
       " [['kushiesonline'], 1],\n",
       " [['laterooms'], 1],\n",
       " [['lego'], 1],\n",
       " [['leprix'], 0],\n",
       " [['liquidation'], 1],\n",
       " [['listia'], 0],\n",
       " [['lonelyplanet'], 1],\n",
       " [['lookfantastic'], 1],\n",
       " [['maccosmetics'], 0],\n",
       " [['macys'], 1],\n",
       " [['marriott'], 0],\n",
       " [['marykay'], 1],\n",
       " [['mercari'], 0],\n",
       " [['metmuseum'], 1],\n",
       " [['microcenter'], 0],\n",
       " [['momondo'], 1],\n",
       " [['mrandmrssmith'], 1],\n",
       " [['mynavyexchange'], 1],\n",
       " [['nba'], 1],\n",
       " [['neutrogena'], 0],\n",
       " [['newbalance'], 1],\n",
       " [['nflshop'], 1],\n",
       " [['nhc'], 1],\n",
       " [['nike'], 1],\n",
       " [['officedepot'], 0],\n",
       " [['officesupplynow'], 1],\n",
       " [['oncewed'], 0],\n",
       " [['onetravel'], 1],\n",
       " [['onlinesports'], 1],\n",
       " [['oodle'], 0],\n",
       " [['orbitz'], 1],\n",
       " [['origins'], 0],\n",
       " [['overstock'], 1],\n",
       " [['oyorooms'], 1],\n",
       " [['petco'], 1],\n",
       " [['petsmart'], 1],\n",
       " [['petsupplies'], 1],\n",
       " [['petsuppliesplus'], 1],\n",
       " [['poppin'], 1],\n",
       " [['poshmark'], 1],\n",
       " [['powells'], 0],\n",
       " [['premierinn'], 1],\n",
       " [['preownedweddingdresses'], 0],\n",
       " [['priceline'], 1],\n",
       " [['pruvo'], 0],\n",
       " [['puma'], 0],\n",
       " [['redbubble'], 1],\n",
       " [['reebok'], 1],\n",
       " [['riteaid'], 0],\n",
       " [['roomertravel'], 1],\n",
       " [['rubylane'], 1],\n",
       " [['saatchiart'], 0],\n",
       " [['safeway'], 1],\n",
       " [['sears'], 1],\n",
       " [['sebamedusa'], 1],\n",
       " [['secondsale'], 1],\n",
       " [['sellcell'], 0],\n",
       " [['sephora'], 1],\n",
       " [['shopbeergear'], 0],\n",
       " [['shopbop'], 0],\n",
       " [['shopdisney'], 1],\n",
       " [['shopgoodwill'], 1],\n",
       " [['shopjustice'], 1],\n",
       " [['shopthesalvationarmy'], 1],\n",
       " [['shopzilla'], 0],\n",
       " [['shutterstock'], 1],\n",
       " [['sisley'], 1],\n",
       " [['skyscanner'], 1],\n",
       " [['spalding'], 1],\n",
       " [['sportsdirect'], 0],\n",
       " [['sportsunlimitedinc'], 0],\n",
       " [['staples'], 0],\n",
       " [['swansonvitamins'], 0],\n",
       " [['sykescottages'], 0],\n",
       " [['szallas'], 1],\n",
       " [['tabasco'], 0],\n",
       " [['tablethotels'], 1],\n",
       " [['target'], 1],\n",
       " [['tenthousandvillages'], 1],\n",
       " [['thellegance'], 0],\n",
       " [['thenewyorkdogshop'], 1],\n",
       " [['thomascook'], 0],\n",
       " [['thredup'], 1],\n",
       " [['thriftbooks'], 0],\n",
       " [['tias'], 1],\n",
       " [['tigerdirect'], 0],\n",
       " [['t-mobile'], 1],\n",
       " [['toofaced'], 0],\n",
       " [['totalwine'], 1],\n",
       " [['tradesy'], 1],\n",
       " [['travelocity'], 1],\n",
       " [['travelzoo'], 1],\n",
       " [['tripadvisor'], 1],\n",
       " [['trivago'], 1],\n",
       " [['usps'], 0],\n",
       " [['veneretravel'], 0],\n",
       " [['verizon'], 1],\n",
       " [['vitaminshoppe'], 1],\n",
       " [['vitaminworld'], 1],\n",
       " [['vrbo'], 1],\n",
       " [['walgreens'], 1],\n",
       " [['walmart'], 0],\n",
       " [['wayfair'], 1],\n",
       " [['wotif'], 1],\n",
       " [['wyndhamhotels'], 1]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, searchCount= 1,0\n",
    "newList = []\n",
    "for i in range(0,209):\n",
    "    name, formList = Extract(urllist[i])\n",
    "    count +=1 \n",
    "    \n",
    "    if (bool(re.search('data-attribute', str(formList))))==True:\n",
    "        searchCount+=1\n",
    "        newList.append([name,1])\n",
    "    else:\n",
    "        newList.append([name,0])\n",
    "newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac776076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_CSV(completeName, className):\n",
    "    df = pd.read_csv(completeName)\n",
    "    falseSearch = (df[className]==1).value_counts()[0]\n",
    "    trueSearch = (df[className]==1).value_counts()[1]\n",
    "    total = len(df[className])\n",
    "    print(className.split('Class')[0])\n",
    "    print(\"================================\")\n",
    "    print('Accuracy: {:.1f}%'.format(((total-falseSearch)/total)*100))\n",
    "    print('Number of False result: {:.1f}'.format(falseSearch))\n",
    "    print('Number of True result: {:.1f}'.format(trueSearch))\n",
    "    print('Number of Total result: {:.1f}'.format(total))\n",
    "def main():\n",
    "    save_path = '/Users/mdjavedulferdous/Desktop/TiiS/Code/'\n",
    "    search_name = \"search.csv\"\n",
    "    page_name = 'page.csv'\n",
    "    sort_name = 'sort.csv'\n",
    "    filter_name = 'filter.csv'\n",
    "    searchName = os.path.join(save_path, search_name)\n",
    "    pageName = os.path.join(save_path, page_name)\n",
    "    sortName = os.path.join(save_path, sort_name)\n",
    "    filterName = os.path.join(save_path, filter_name)\n",
    "    Read_CSV(searchName,'sClass')\n",
    "    Read_CSV(pageName,'pageClass')\n",
    "    Read_CSV(sortName,'sortClass')\n",
    "    Read_CSV(filterName,'filterClass')\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec61841",
   "metadata": {},
   "source": [
    "#### Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaaeecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import clear\n",
    "from selenium import webdriver\n",
    "\n",
    "DRIVER_PATH = '/Users/mdjavedulferdous/Desktop/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "URL1 = 'https://www.amazon.com/s?k=apple+watch&i=electronics&crid=3MR9PVJ318602&sprefix=%2Celectronics%2C328&ref=nb_sb_ss_recent_1_0_recent'\n",
    "URL2 = 'https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2380057.m570.l1313&_nkw=laptop&_sacat=175672'\n",
    "def ExtractLive(_URL_):\n",
    "    driver.get(_URL_)\n",
    "    #print(driver.page_source)\n",
    "    soup_level1=BeautifulSoup(driver.page_source, 'lxml')\n",
    "    name, formList = [], []\n",
    "\n",
    "    try:\n",
    "                for tests in soup_level1.findAll('form'):\n",
    "                    my_attributes = tests.attrs\n",
    "                    print(my_attributes)                \n",
    "                    if my_attributes.has_attr('action')==True:\n",
    "                        pass #print(my_attributes)  \n",
    "\n",
    "    except:\n",
    "                pass\n",
    "ExtractLive(URL1)\n",
    "ExtractLive(URL2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294fdbe",
   "metadata": {},
   "source": [
    "### Extract Sort node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract(_URL_):\n",
    "    listOfTag = [ \"div\",'select', 'ul']\n",
    "    myFile=open(_URL_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    name, formList = [], []\n",
    "    name.append(urllist[i].split('/')[-2])\n",
    "    try:\n",
    "            for tests in soup.findAll('select'):\n",
    "                if bool(re.search('sort', str(tests))) ==True:\n",
    "                    print(name,tests)\n",
    "                \n",
    "    except:\n",
    "            pass\n",
    "    return name, formList\n",
    "\n",
    "count= 1\n",
    "for i in range(0,20):\n",
    "    name, formList = Extract(urllist[i])\n",
    "    count +=1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5097271",
   "metadata": {},
   "source": [
    "### Extract Page node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34649b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def search(myDict, search1):\n",
    "    search.a=[]\n",
    "    for key, value in myDict.items():\n",
    "        if search1 in value:\n",
    "            search.a.append(key)\n",
    "    return len(search.a)\n",
    "\n",
    "def Extract(_URL_):\n",
    "    listOfTag = [ \"div\", \"nav\",\"li\",\"ul\",\"span\", \"section\", \"button\", \"tr\", \"footer\", \"a\", \"pagination\", \"b\" ]\n",
    "    myFile=open(_URL_,'r',encoding=\"latin-1\")\n",
    "    soup=BeautifulSoup(myFile,\"html5lib\")\n",
    "    name, pageList = [], []\n",
    "    name.append(urllist[i].split('/')[-2])\n",
    "    try:\n",
    "        for j in listOfTag:\n",
    "            for tests in soup.findAll(j):\n",
    "                    my_attributes = tests.attrs\n",
    "                    titles = soup.find_all('div',attrs = {'span'})\n",
    "                    print(titles)\n",
    "                    if my_attributes != \" \":\n",
    "                        if tests.find('page')!= -1:\n",
    "                            #print(name,j,list(tests))\n",
    "                            pageList.append(my_attributes)\n",
    "\n",
    "    except:\n",
    "            pass\n",
    "    return name, pageList\n",
    "\n",
    "count= 1\n",
    "for i in range(6,7):\n",
    "    name, pageList = Extract(urllist[i])\n",
    "    count +=1 \n",
    "    #print(name, pageList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c153f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
